"""
å‘é‡ç¼“å­˜ç®¡ç†å™¨ - æœ¬åœ°å­˜å‚¨å‘é‡ï¼Œé¿å…é‡å¤è®¡ç®—
"""
import os
import json
import pickle
import hashlib
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
import time

from config import settings


class VectorCache:
    """å‘é‡ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None):
        self.cache_dir = Path(cache_dir or os.path.join(settings.DATA_DIR, "vector_cache"))
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.vectors_file = self.cache_dir / "vectors.pkl"
        self.metadata_file = self.cache_dir / "metadata.json"
        self.index_file = self.cache_dir / "text_index.json"
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        self.vectors_cache = self._load_vectors()
        self.metadata_cache = self._load_metadata()
        self.text_index = self._load_text_index()
        
        print(f"ğŸ“¦ å‘é‡ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"ğŸ“Š å½“å‰ç¼“å­˜: {len(self.vectors_cache)} ä¸ªå‘é‡")
    
    def _load_vectors(self) -> Dict[str, np.ndarray]:
        """åŠ è½½å‘é‡ç¼“å­˜"""
        if self.vectors_file.exists():
            try:
                with open(self.vectors_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _load_metadata(self) -> Dict[str, Dict]:
        """åŠ è½½å…ƒæ•°æ®ç¼“å­˜"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å…ƒæ•°æ®ç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _load_text_index(self) -> Dict[str, str]:
        """åŠ è½½æ–‡æœ¬ç´¢å¼•"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ–‡æœ¬ç´¢å¼•å¤±è´¥: {e}")
        return {}
    
    def _save_vectors(self):
        """ä¿å­˜å‘é‡ç¼“å­˜"""
        try:
            with open(self.vectors_file, 'wb') as f:
                pickle.dump(self.vectors_cache, f)
        except Exception as e:
            print(f"âŒ ä¿å­˜å‘é‡ç¼“å­˜å¤±è´¥: {e}")
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®ç¼“å­˜"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜å…ƒæ•°æ®ç¼“å­˜å¤±è´¥: {e}")
    
    def _save_text_index(self):
        """ä¿å­˜æ–‡æœ¬ç´¢å¼•"""
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.text_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡æœ¬ç´¢å¼•å¤±è´¥: {e}")
    
    def _get_text_hash(self, text: str, model_name: str = "default") -> str:
        """ç”Ÿæˆæ–‡æœ¬å“ˆå¸Œå€¼"""
        # åŒ…å«æ¨¡å‹åç§°ï¼Œç¡®ä¿ä¸åŒæ¨¡å‹çš„å‘é‡ä¸ä¼šæ··æ·†
        content = f"{model_name}:{text}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get_cached_vectors(self, texts: List[str], model_name: str = "default") -> tuple:
        """
        è·å–ç¼“å­˜çš„å‘é‡
        
        Returns:
            (cached_vectors, missing_texts, missing_indices)
        """
        cached_vectors = []
        missing_texts = []
        missing_indices = []
        
        for i, text in enumerate(texts):
            text_hash = self._get_text_hash(text, model_name)
            
            if text_hash in self.vectors_cache:
                cached_vectors.append(self.vectors_cache[text_hash])
            else:
                cached_vectors.append(None)
                missing_texts.append(text)
                missing_indices.append(i)
        
        hit_rate = (len(texts) - len(missing_texts)) / len(texts) * 100
        print(f"ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}% ({len(texts) - len(missing_texts)}/{len(texts)})")
        
        return cached_vectors, missing_texts, missing_indices
    
    def cache_vectors(self, texts: List[str], vectors: np.ndarray, model_name: str = "default"):
        """ç¼“å­˜å‘é‡"""
        if len(texts) != len(vectors):
            raise ValueError("æ–‡æœ¬æ•°é‡å’Œå‘é‡æ•°é‡ä¸åŒ¹é…")
        
        cached_count = 0
        for text, vector in zip(texts, vectors):
            text_hash = self._get_text_hash(text, model_name)
            
            # ç¼“å­˜å‘é‡
            self.vectors_cache[text_hash] = vector
            
            # ç¼“å­˜å…ƒæ•°æ®
            self.metadata_cache[text_hash] = {
                'model_name': model_name,
                'vector_dim': len(vector),
                'cached_time': time.time(),
                'text_length': len(text)
            }
            
            # ç¼“å­˜æ–‡æœ¬ç´¢å¼•ï¼ˆç”¨äºè°ƒè¯•å’ŒæŸ¥çœ‹ï¼‰
            self.text_index[text_hash] = text[:100] + "..." if len(text) > 100 else text
            
            cached_count += 1
        
        # ä¿å­˜åˆ°ç£ç›˜
        self._save_vectors()
        self._save_metadata()
        self._save_text_index()
        
        print(f"ğŸ’¾ å·²ç¼“å­˜ {cached_count} ä¸ªæ–°å‘é‡")
    
    def get_or_compute_vectors(self, texts: List[str], compute_func, model_name: str = "default") -> np.ndarray:
        """
        è·å–æˆ–è®¡ç®—å‘é‡ï¼ˆæ™ºèƒ½ç¼“å­˜ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            compute_func: è®¡ç®—å‘é‡çš„å‡½æ•°
            model_name: æ¨¡å‹åç§°
            
        Returns:
            å‘é‡æ•°ç»„
        """
        print(f"ğŸ” æ£€æŸ¥ {len(texts)} ä¸ªæ–‡æœ¬çš„å‘é‡ç¼“å­˜...")
        
        # æ£€æŸ¥ç¼“å­˜
        cached_vectors, missing_texts, missing_indices = self.get_cached_vectors(texts, model_name)
        
        if not missing_texts:
            print("âœ… æ‰€æœ‰å‘é‡éƒ½å·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›")
            return np.array([v for v in cached_vectors if v is not None])
        
        print(f"ğŸ”„ éœ€è¦è®¡ç®— {len(missing_texts)} ä¸ªæ–°å‘é‡...")
        
        # è®¡ç®—ç¼ºå¤±çš„å‘é‡
        new_vectors = compute_func(missing_texts)
        
        # ç¼“å­˜æ–°å‘é‡
        self.cache_vectors(missing_texts, new_vectors, model_name)
        
        # åˆå¹¶ç»“æœ
        result_vectors = []
        new_vector_idx = 0
        
        for i, cached_vector in enumerate(cached_vectors):
            if cached_vector is not None:
                result_vectors.append(cached_vector)
            else:
                result_vectors.append(new_vectors[new_vector_idx])
                new_vector_idx += 1
        
        return np.array(result_vectors)
    
    def clear_cache(self, model_name: str = None):
        """æ¸…ç©ºç¼“å­˜"""
        if model_name:
            # æ¸…ç©ºç‰¹å®šæ¨¡å‹çš„ç¼“å­˜
            to_remove = []
            for text_hash, metadata in self.metadata_cache.items():
                if metadata.get('model_name') == model_name:
                    to_remove.append(text_hash)
            
            for text_hash in to_remove:
                self.vectors_cache.pop(text_hash, None)
                self.metadata_cache.pop(text_hash, None)
                self.text_index.pop(text_hash, None)
            
            print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ¨¡å‹ {model_name} çš„ç¼“å­˜ ({len(to_remove)} ä¸ªå‘é‡)")
        else:
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            count = len(self.vectors_cache)
            self.vectors_cache.clear()
            self.metadata_cache.clear()
            self.text_index.clear()
            print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ ({count} ä¸ªå‘é‡)")
        
        # ä¿å­˜æ›´æ”¹
        self._save_vectors()
        self._save_metadata()
        self._save_text_index()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.metadata_cache:
            return {
                'total_vectors': 0,
                'models': {},
                'cache_size_mb': 0,
                'oldest_cache': None,
                'newest_cache': None
            }
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        models = {}
        cache_times = []
        
        for metadata in self.metadata_cache.values():
            model_name = metadata.get('model_name', 'unknown')
            if model_name not in models:
                models[model_name] = 0
            models[model_name] += 1
            cache_times.append(metadata.get('cached_time', 0))
        
        # è®¡ç®—ç¼“å­˜æ–‡ä»¶å¤§å°
        cache_size = 0
        for file_path in [self.vectors_file, self.metadata_file, self.index_file]:
            if file_path.exists():
                cache_size += file_path.stat().st_size
        
        return {
            'total_vectors': len(self.vectors_cache),
            'models': models,
            'cache_size_mb': cache_size / 1024 / 1024,
            'oldest_cache': time.ctime(min(cache_times)) if cache_times else None,
            'newest_cache': time.ctime(max(cache_times)) if cache_times else None,
            'cache_dir': str(self.cache_dir)
        }
    
    def export_cache_info(self) -> str:
        """å¯¼å‡ºç¼“å­˜ä¿¡æ¯"""
        stats = self.get_cache_stats()
        
        info = f"""
ğŸ“¦ å‘é‡ç¼“å­˜ä¿¡æ¯
{'='*50}
æ€»å‘é‡æ•°: {stats['total_vectors']}
ç¼“å­˜å¤§å°: {stats['cache_size_mb']:.2f} MB
ç¼“å­˜ç›®å½•: {stats['cache_dir']}

ğŸ“Š æŒ‰æ¨¡å‹ç»Ÿè®¡:
"""
        for model, count in stats['models'].items():
            info += f"  â€¢ {model}: {count} ä¸ªå‘é‡\n"
        
        if stats['oldest_cache']:
            info += f"\nâ° æœ€æ—©ç¼“å­˜: {stats['oldest_cache']}"
            info += f"\nâ° æœ€æ–°ç¼“å­˜: {stats['newest_cache']}"
        
        return info
