"""
APIå‘é‡åŒ–ç®¡ç†å™¨ - æ”¯æŒå¤šç§å…è´¹API
"""
import requests
import numpy as np
from typing import List, Dict, Any
import time
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import chromadb
from chromadb.config import Settings as ChromaSettings

from config import settings
from vector_cache import VectorCache


class APIEmbeddingManager:
    """APIå‘é‡åŒ–ç®¡ç†å™¨"""

    def __init__(self, api_provider: str = "huggingface", enable_cache: bool = True):
        self.api_provider = api_provider
        self.enable_cache = enable_cache
        self.chroma_client = None
        self.collection = None
        self.api_configs = self._setup_api_configs()

        # åˆå§‹åŒ–å‘é‡ç¼“å­˜
        if enable_cache:
            self.vector_cache = VectorCache()
            print("ğŸ’¾ APIå‘é‡ç¼“å­˜å·²å¯ç”¨")
        else:
            self.vector_cache = None
            print("âš ï¸ APIå‘é‡ç¼“å­˜å·²ç¦ç”¨")

        self._init_vector_db()

    def _setup_api_configs(self) -> Dict[str, Dict]:
        """è®¾ç½®APIé…ç½®"""
        return {
            "huggingface": {
                "url": "https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "headers": {"Authorization": f"Bearer {os.getenv('HUGGINGFACE_API_KEY', '')}"},
                "free": True,
                "rate_limit": 1000,  # æ¯å°æ—¶è¯·æ±‚æ•°
                "batch_size": 50
            },
            "cohere": {
                "url": "https://api.cohere.ai/v1/embed",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('COHERE_API_KEY', '')}",
                    "Content-Type": "application/json"
                },
                "free": True,
                "rate_limit": 100,  # æ¯æœˆå…è´¹é¢åº¦
                "batch_size": 96
            },
            "openai": {
                "url": "https://api.openai.com/v1/embeddings",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY', '')}",
                    "Content-Type": "application/json"
                },
                "free": False,  # ä»˜è´¹ä½†ä¾¿å®œ
                "rate_limit": 3000,
                "batch_size": 100
            },
            "jina": {
                "url": "https://api.jina.ai/v1/embeddings",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('JINA_API_KEY', '')}",
                    "Content-Type": "application/json"
                },
                "free": True,  # æœ‰å…è´¹é¢åº¦
                "rate_limit": 1000,
                "batch_size": 100
            }
        }

    def _init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        print("æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=settings.VECTOR_DB_DIR,
                settings=ChromaSettings(anonymized_telemetry=False)
            )

            self.collection = self.chroma_client.get_or_create_collection(
                name="huatuo_medical_qa_api",
                metadata={"description": "åä½—åŒ»å­¦çŸ¥è¯†å›¾è°±QAå‘é‡é›†åˆ(APIç‰ˆ)"}
            )
            print("å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def encode_texts_huggingface(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨HuggingFace APIç¼–ç """
        config = self.api_configs["huggingface"]

        def query_api(payload):
            response = requests.post(config["url"], headers=config["headers"], json=payload)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"HuggingFace APIé”™è¯¯: {response.status_code} - {response.text}")
                return None

        all_embeddings = []
        batch_size = config["batch_size"]

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            print(f"ğŸŒ HuggingFace APIå¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")

            result = query_api({"inputs": batch_texts})
            if result:
                all_embeddings.extend(result)

            # é¿å…APIé™åˆ¶
            time.sleep(1)

        return np.array(all_embeddings)

    def encode_texts_cohere(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨Cohere APIç¼–ç """
        config = self.api_configs["cohere"]

        payload = {
            "texts": texts,
            "model": "embed-multilingual-v2.0",
            "input_type": "search_document"
        }

        print(f"ğŸŒ Cohere APIå¤„ç† {len(texts)} ä¸ªæ–‡æœ¬...")
        response = requests.post(config["url"], headers=config["headers"], json=payload)

        if response.status_code == 200:
            result = response.json()
            return np.array(result["embeddings"])
        else:
            print(f"Cohere APIé”™è¯¯: {response.status_code} - {response.text}")
            raise Exception(f"Cohere APIè°ƒç”¨å¤±è´¥: {response.status_code}")

    def encode_texts_jina(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨Jina AI APIç¼–ç """
        config = self.api_configs["jina"]

        payload = {
            "input": texts,
            "model": "jina-embeddings-v2-base-zh"  # æ”¯æŒä¸­æ–‡
        }

        print(f"ğŸŒ Jina AI APIå¤„ç† {len(texts)} ä¸ªæ–‡æœ¬...")
        response = requests.post(config["url"], headers=config["headers"], json=payload)

        if response.status_code == 200:
            result = response.json()
            embeddings = [item["embedding"] for item in result["data"]]
            return np.array(embeddings)
        else:
            print(f"Jina AI APIé”™è¯¯: {response.status_code} - {response.text}")
            raise Exception(f"Jina AI APIè°ƒç”¨å¤±è´¥: {response.status_code}")

    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
        try:
            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œä½¿ç”¨æ™ºèƒ½ç¼“å­˜
            if self.enable_cache and self.vector_cache:
                return self.vector_cache.get_or_compute_vectors(
                    texts,
                    lambda missing_texts: self._compute_api_embeddings(missing_texts),
                    f"api_{self.api_provider}"
                )
            else:
                # ç›´æ¥è°ƒç”¨APIè®¡ç®—å‘é‡
                return self._compute_api_embeddings(texts)

        except Exception as e:
            print(f"APIå‘é‡åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¤‡ç”¨çš„æœ¬åœ°æ¨¡å‹
            raise

    def _compute_api_embeddings(self, texts: List[str]) -> np.ndarray:
        """å®é™…è°ƒç”¨APIè®¡ç®—å‘é‡"""
        print(f"ğŸŒ ä½¿ç”¨ {self.api_provider} APIè¿›è¡Œå‘é‡åŒ–...")

        if self.api_provider == "huggingface":
            return self.encode_texts_huggingface(texts)
        elif self.api_provider == "cohere":
            return self.encode_texts_cohere(texts)
        elif self.api_provider == "jina":
            return self.encode_texts_jina(texts)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„APIæä¾›å•†: {self.api_provider}")

    def add_documents(self, chunks: List[Dict[str, Any]]) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        if not chunks:
            print("æ²¡æœ‰æ–‡æ¡£éœ€è¦æ·»åŠ ")
            return

        print(f"ğŸŒ ä½¿ç”¨APIå‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æ¡£å—...")

        # å‡†å¤‡æ•°æ®
        texts = [chunk['text'] for chunk in chunks]
        ids = [chunk['chunk_id'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]

        # APIå‘é‡åŒ–
        embeddings = self.encode_texts(texts)

        # æ™ºèƒ½è°ƒæ•´æ•°æ®åº“æ‰¹å¤„ç†å¤§å°
        total_chunks = len(chunks)
        if total_chunks > 50000:
            batch_size = 100  # è¶…å¤§æ•°æ®é›†ç”¨å°æ‰¹æ¬¡
        elif total_chunks > 10000:
            batch_size = 200  # å¤§æ•°æ®é›†ç”¨ä¸­ç­‰æ‰¹æ¬¡
        elif total_chunks > 1000:
            batch_size = 300  # ä¸­ç­‰æ•°æ®é›†
        else:
            batch_size = 500  # å°æ•°æ®é›†ç”¨å¤§æ‰¹æ¬¡

        print(f"ğŸ“Š APIæ•°æ®åº“æ‰¹å¤„ç†å¤§å°: {batch_size} (æ€»æ•°æ®: {total_chunks})")

        for i in range(0, len(chunks), batch_size):
            end_idx = min(i + batch_size, len(chunks))

            batch_texts = texts[i:end_idx]
            batch_ids = ids[i:end_idx]
            batch_embeddings = embeddings[i:end_idx].tolist()
            batch_metadatas = metadatas[i:end_idx]

            try:
                self.collection.add(
                    documents=batch_texts,
                    embeddings=batch_embeddings,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                print(f"âœ… å·²æ·»åŠ æ‰¹æ¬¡ {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}")
            except Exception as e:
                print(f"âŒ æ·»åŠ æ‰¹æ¬¡å¤±è´¥: {e}")
                continue

        print("ğŸ‰ APIå‘é‡åŒ–å®Œæˆï¼")

    def search_similar(self, query: str, top_k: int = None, threshold: float = None) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        top_k = top_k or settings.TOP_K_RETRIEVAL
        threshold = threshold or settings.SIMILARITY_THRESHOLD

        if not self.collection:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        try:
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
            query_embedding = self.encode_texts([query])[0].tolist()

            # æœç´¢ç›¸ä¼¼æ–‡æ¡£
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=['documents', 'metadatas', 'distances']
            )

            # å¤„ç†ç»“æœ
            similar_docs = []
            if results['documents'] and results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    similarity = 1 - distance
                    if similarity >= threshold:
                        similar_docs.append({
                            'text': doc,
                            'metadata': metadata,
                            'similarity': similarity,
                            'rank': i + 1
                        })

            return similar_docs

        except Exception as e:
            print(f"ç›¸ä¼¼æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            return []

    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            return {}

        try:
            count = self.collection.count()
            return {
                'total_documents': count,
                'collection_name': self.collection.name,
                'api_provider': self.api_provider,
                'embedding_type': 'API'
            }
        except Exception as e:
            print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
