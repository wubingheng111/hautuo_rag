"""
åµŒå…¥æ¨¡å‹å¤„ç†æ¨¡å—
"""
import numpy as np
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings as ChromaSettings
import os

from config import settings
from vector_cache import VectorCache

# å°è¯•å¯¼å…¥sentence_transformersï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
from sentence_transformers import SentenceTransformer
SENTENCE_TRANSFORMERS_AVAILABLE = True
print("âœ… sentence-transformers å¯ç”¨")


class EmbeddingManager:
    """åµŒå…¥ç®¡ç†å™¨"""

    def __init__(self, enable_cache: bool = True):
        self.model = None
        self.chroma_client = None
        self.collection = None
        self.use_lite_mode = False
        self.enable_cache = enable_cache

        # åˆå§‹åŒ–å‘é‡ç¼“å­˜
        if enable_cache:
            self.vector_cache = VectorCache()
            print("ğŸ’¾ å‘é‡ç¼“å­˜å·²å¯ç”¨")
        else:
            self.vector_cache = None
            print("âš ï¸ å‘é‡ç¼“å­˜å·²ç¦ç”¨")

        # å¦‚æœsentence_transformersä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡çº§æ¨¡å¼
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡çº§åµŒå…¥æ¨¡å¼")
            self._init_lite_mode()
        else:
            try:
                self._load_embedding_model()
            except Exception as e:
                print(f"âš ï¸ æ ‡å‡†åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ åˆ‡æ¢åˆ°è½»é‡çº§åµŒå…¥æ¨¡å¼")
                self._init_lite_mode()

        self._init_vector_db()

    def _init_lite_mode(self):
        """åˆå§‹åŒ–è½»é‡çº§æ¨¡å¼"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            import jieba

            self.use_lite_mode = True
            self.vectorizer = TfidfVectorizer(
                max_features=384,  # åŒ¹é…åŸå§‹embeddingç»´åº¦
                tokenizer=self._chinese_tokenizer,
                lowercase=False,
                ngram_range=(1, 2)
            )
            print("âœ… TF-IDFè½»é‡çº§æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ è½»é‡çº§æ¨¡å¼ä¹Ÿæ— æ³•åˆå§‹åŒ–: {e}")
            raise Exception("æ— æ³•åˆå§‹åŒ–ä»»ä½•åµŒå…¥æ¨¡å¼ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")

    def _chinese_tokenizer(self, text):
        """ä¸­æ–‡åˆ†è¯å™¨"""
        try:
            import jieba
            return list(jieba.cut(text))
        except:
            # å¦‚æœjiebaä¹Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦åˆ†å‰²
            return list(text)

    def _load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {settings.EMBEDDING_MODEL}")
        try:
            # ä½¿ç”¨ç¼“å­˜ç›®å½•åŠ é€Ÿæ¨¡å‹åŠ è½½
            cache_dir = os.path.join(settings.DATA_DIR, "model_cache")
            os.makedirs(cache_dir, exist_ok=True)

            self.model = SentenceTransformer(settings.EMBEDDING_MODEL)

            # æ£€æŸ¥å¹¶è®¾ç½®è®¾å¤‡
            import torch
            if torch.cuda.is_available():
                device = 'cuda'
                gpu_name = torch.cuda.get_device_name(0)
                print(f"ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name}")
            else:
                device = 'cpu'
                print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")

            # å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU
            self.model = self.model.to(device)
            print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
        except Exception as e:
            print(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¤‡ç”¨æ¨¡å‹
            print("å°è¯•åŠ è½½å¤‡ç”¨æ¨¡å‹...")
            try:
                cache_dir = os.path.join(settings.DATA_DIR, "model_cache")
                self.model = SentenceTransformer('all-MiniLM-L6-v2')

                # å¤‡ç”¨æ¨¡å‹ä¹Ÿè¦ç§»åŠ¨åˆ°GPU
                import torch
                if torch.cuda.is_available():
                    device = 'cuda'
                    gpu_name = torch.cuda.get_device_name(0)
                    print(f"ğŸš€ å¤‡ç”¨æ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name}")
                else:
                    device = 'cpu'
                    print("ğŸ’» å¤‡ç”¨æ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")

                self.model = self.model.to(device)
                print(f"âœ… å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
            except Exception as e2:
                print(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                raise

    def _init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        print("æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=settings.VECTOR_DB_DIR,
                settings=ChromaSettings(anonymized_telemetry=False)
            )

            # åˆ›å»ºæˆ–è·å–é›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="huatuo_medical_qa",
                metadata={"description": "åä½—åŒ»å­¦çŸ¥è¯†å›¾è°±QAå‘é‡é›†åˆ"}
            )
            print("å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def encode_texts(self, texts: List[str], batch_size: int = None) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ï¼ˆæ”¯æŒç¼“å­˜ï¼‰

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            å‘é‡æ•°ç»„
        """
        try:
            # è·å–æ¨¡å‹åç§°ç”¨äºç¼“å­˜
            model_name = "tfidf" if self.use_lite_mode else getattr(self.model, 'model_name', settings.EMBEDDING_MODEL)

            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œä½¿ç”¨æ™ºèƒ½ç¼“å­˜
            if self.enable_cache and self.vector_cache:
                return self.vector_cache.get_or_compute_vectors(
                    texts,
                    lambda missing_texts: self._compute_embeddings(missing_texts, batch_size),
                    model_name
                )
            else:
                # ç›´æ¥è®¡ç®—å‘é‡
                return self._compute_embeddings(texts, batch_size)

        except Exception as e:
            print(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            raise

    def _compute_embeddings(self, texts: List[str], batch_size: int = None) -> np.ndarray:
        """å®é™…è®¡ç®—å‘é‡çš„æ–¹æ³•"""
        if self.use_lite_mode:
            # ä½¿ç”¨TF-IDFæ¨¡å¼
            if not hasattr(self.vectorizer, 'vocabulary_'):
                print("é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨è®­ç»ƒTF-IDFæ¨¡å‹...")
                embeddings = self.vectorizer.fit_transform(texts)
            else:
                embeddings = self.vectorizer.transform(texts)

            # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ
            dense_embeddings = embeddings.toarray()

            # å½’ä¸€åŒ–
            norms = np.linalg.norm(dense_embeddings, axis=1, keepdims=True)
            norms[norms == 0] = 1  # é¿å…é™¤é›¶
            normalized_embeddings = dense_embeddings / norms

            return normalized_embeddings
        else:
            # ä½¿ç”¨sentence_transformersæ¨¡å¼
            if not self.model:
                raise ValueError("åµŒå…¥æ¨¡å‹æœªåŠ è½½")

            # æ ¹æ®è®¾å¤‡å’Œæ–‡æœ¬æ•°é‡è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°
            if batch_size is None:
                import torch
                if torch.cuda.is_available() and self.model.device.type == 'cuda':
                    # GPUæ¨¡å¼ï¼šä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°
                    batch_size = min(256, len(texts))  # æœ€å¤§256ï¼Œé¿å…æ˜¾å­˜ä¸è¶³
                    print(f"ğŸš€ GPUæ¨¡å¼ï¼Œè®¾å¤‡: {self.model.device}ï¼Œæ‰¹å¤„ç†å¤§å°: {batch_size}")
                else:
                    # CPUæ¨¡å¼ï¼šä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°
                    batch_size = min(64, len(texts))
                    print(f"ğŸ’» CPUæ¨¡å¼ï¼Œè®¾å¤‡: {self.model.device}ï¼Œæ‰¹å¤„ç†å¤§å°: {batch_size}")

            # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            import torch
            if torch.cuda.is_available() and self.model.device.type == 'cuda':
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨: å·²åˆ†é… {memory_allocated:.1f}GB, å·²ç¼“å­˜ {memory_cached:.1f}GB")

            # ä½¿ç”¨ä¼˜åŒ–å‚æ•°åŠ é€Ÿç¼–ç 
            embeddings = self.model.encode(
                texts,
                show_progress_bar=True,
                batch_size=batch_size,
                convert_to_numpy=True,
                normalize_embeddings=True,  # å½’ä¸€åŒ–å‘é‡ï¼Œæé«˜æ£€ç´¢æ•ˆæœ
                device=self.model.device  # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
            )
            return embeddings

    def add_documents(self, chunks: List[Dict[str, Any]]) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“

        Args:
            chunks: æ–‡æ¡£å—åˆ—è¡¨
        """
        if not chunks:
            print("æ²¡æœ‰æ–‡æ¡£éœ€è¦æ·»åŠ ")
            return

        print(f"æ­£åœ¨æ·»åŠ  {len(chunks)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“...")

        # å‡†å¤‡æ•°æ®
        texts = [chunk['text'] for chunk in chunks]
        ids = [chunk['chunk_id'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]

        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = self.encode_texts(texts)

        # æ™ºèƒ½è°ƒæ•´æ•°æ®åº“æ‰¹å¤„ç†å¤§å°
        total_chunks = len(chunks)
        if total_chunks > 50000:
            db_batch_size = 100  # è¶…å¤§æ•°æ®é›†ç”¨å°æ‰¹æ¬¡
        elif total_chunks > 10000:
            db_batch_size = 200  # å¤§æ•°æ®é›†ç”¨ä¸­ç­‰æ‰¹æ¬¡
        elif total_chunks > 1000:
            db_batch_size = 300  # ä¸­ç­‰æ•°æ®é›†
        else:
            db_batch_size = 500  # å°æ•°æ®é›†ç”¨å¤§æ‰¹æ¬¡

        print(f"ğŸ“Š æ•°æ®åº“æ‰¹å¤„ç†å¤§å°: {db_batch_size} (æ€»æ•°æ®: {total_chunks})")

        for i in range(0, len(chunks), db_batch_size):
            end_idx = min(i + db_batch_size, len(chunks))

            batch_texts = texts[i:end_idx]
            batch_ids = ids[i:end_idx]
            batch_embeddings = embeddings[i:end_idx].tolist()
            batch_metadatas = metadatas[i:end_idx]

            try:
                self.collection.add(
                    documents=batch_texts,
                    embeddings=batch_embeddings,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                print(f"å·²æ·»åŠ æ‰¹æ¬¡ {i//db_batch_size + 1}/{(len(chunks)-1)//db_batch_size + 1}")
            except Exception as e:
                print(f"æ·»åŠ æ‰¹æ¬¡å¤±è´¥: {e}")
                continue

        print("æ–‡æ¡£æ·»åŠ å®Œæˆ")

    def search_similar(
        self,
        query: str,
        top_k: int = None,
        threshold: float = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        top_k = top_k or settings.TOP_K_RETRIEVAL
        threshold = threshold or settings.SIMILARITY_THRESHOLD

        if not self.collection:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        try:
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
            query_embedding = self.encode_texts([query])[0].tolist()

            # æœç´¢ç›¸ä¼¼æ–‡æ¡£ - å…ˆè·å–æ›´å¤šç»“æœ
            search_k = max(top_k * 3, 50)  # æœç´¢æ›´å¤šç»“æœä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„å€™é€‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=search_k,
                include=['documents', 'metadatas', 'distances']
            )

            # å¤„ç†ç»“æœ
            similar_docs = []
            if results['documents'] and results['documents'][0]:
                print(f"ğŸ” ChromaDBè¿”å› {len(results['documents'][0])} æ¡åŸå§‹ç»“æœ")

                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå‰å‡ ä¸ªç»“æœçš„è·ç¦»
                    if i < 3:
                        print(f"   ç»“æœ {i+1}: è·ç¦»={distance:.4f}")

                    # ChromaDBä½¿ç”¨çš„æ˜¯è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                    # æ ¹æ®è·ç¦»èŒƒå›´åˆ¤æ–­ä½¿ç”¨çš„è·ç¦»ç±»å‹å¹¶ç›¸åº”è½¬æ¢
                    if distance <= 2.0:  # å¯èƒ½æ˜¯ä½™å¼¦è·ç¦»æˆ–å½’ä¸€åŒ–çš„æ¬§å‡ é‡Œå¾—è·ç¦»
                        # å¯¹äºä½™å¼¦è·ç¦»: similarity = 1 - distance
                        # ä½†å¦‚æœæ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œéœ€è¦ä¸åŒçš„è½¬æ¢
                        if distance >= 1.0:  # å¾ˆå¯èƒ½æ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»
                            # ä½¿ç”¨æŒ‡æ•°è¡°å‡è½¬æ¢ï¼šè·ç¦»è¶Šå¤§ï¼Œç›¸ä¼¼åº¦è¶Šå°
                            similarity = max(0, 1 - (distance - 1) * 0.5)  # çº¿æ€§è¡°å‡
                        else:
                            similarity = 1 - distance  # ä½™å¼¦è·ç¦»
                    else:  # å¤§è·ç¦»å€¼ï¼Œä½¿ç”¨å€’æ•°è½¬æ¢
                        similarity = 1 / (1 + distance * 0.5)

                    # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ - æ ¹æ®è·ç¦»åˆ†å¸ƒè‡ªé€‚åº”
                    if i == 0:  # ç¬¬ä¸€ä¸ªç»“æœï¼Œè®¾ç½®åŸºå‡†
                        min_distance = distance
                        # å¦‚æœæœ€å°è·ç¦»éƒ½å¾ˆå¤§ï¼Œè¯´æ˜æ²¡æœ‰å¾ˆç›¸ä¼¼çš„ç»“æœï¼Œé™ä½é˜ˆå€¼
                        if min_distance > 1.5:
                            effective_threshold = 0.01  # æä½é˜ˆå€¼
                        elif min_distance > 1.0:
                            effective_threshold = 0.1   # ä½é˜ˆå€¼
                        else:
                            effective_threshold = min(threshold, 0.3)  # æ­£å¸¸é˜ˆå€¼
                    else:
                        effective_threshold = getattr(locals(), 'effective_threshold', 0.1)

                    if similarity >= effective_threshold:
                        # ç”Ÿæˆæ–‡æ¡£IDï¼ˆå¦‚æœmetadataä¸­æ²¡æœ‰çš„è¯ï¼‰
                        doc_id = metadata.get('id', f"doc_{hash(doc[:100]) % 100000}")

                        similar_docs.append({
                            'id': doc_id,
                            'text': doc,
                            'metadata': metadata,
                            'similarity': similarity,
                            'distance': distance,  # ä¿ç•™åŸå§‹è·ç¦»ç”¨äºè°ƒè¯•
                            'rank': i + 1
                        })

                print(f"ğŸ“Š é˜ˆå€¼ {effective_threshold} è¿‡æ»¤å: {len(similar_docs)} æ¡ç»“æœ")

                # å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œé™ä½é˜ˆå€¼å†è¯•ä¸€æ¬¡
                if not similar_docs and threshold > 0.05:
                    print("ğŸ”„ é™ä½é˜ˆå€¼é‡è¯•...")
                    for i, (doc, metadata, distance) in enumerate(zip(
                        results['documents'][0][:10],  # åªæ£€æŸ¥å‰10ä¸ª
                        results['metadatas'][0][:10],
                        results['distances'][0][:10]
                    )):
                        if distance <= 2.0:
                            similarity = 1 - distance
                        else:
                            similarity = 1 / (1 + distance)

                        if similarity >= 0.01:  # æä½é˜ˆå€¼
                            doc_id = metadata.get('id', f"doc_{hash(doc[:100]) % 100000}")
                            similar_docs.append({
                                'id': doc_id,
                                'text': doc,
                                'metadata': metadata,
                                'similarity': similarity,
                                'distance': distance,
                                'rank': i + 1
                            })

                    print(f"ğŸ†˜ æä½é˜ˆå€¼ 0.01 ç»“æœ: {len(similar_docs)} æ¡")

            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_k
            similar_docs.sort(key=lambda x: x['similarity'], reverse=True)
            return similar_docs[:top_k]

        except Exception as e:
            print(f"ç›¸ä¼¼æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_document_by_id(self, doc_id: str) -> Dict[str, Any]:
        """
        æ ¹æ®æ–‡æ¡£IDè·å–æ–‡æ¡£è¯¦æƒ…

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            æ–‡æ¡£è¯¦æƒ…
        """
        if not self.collection:
            return None

        try:
            # å°è¯•é€šè¿‡IDç›´æ¥æŸ¥è¯¢
            results = self.collection.get(
                ids=[doc_id],
                include=['documents', 'metadatas']
            )

            if results['documents'] and len(results['documents']) > 0:
                return {
                    'id': doc_id,
                    'text': results['documents'][0],
                    'metadata': results['metadatas'][0] if results['metadatas'] else {}
                }

            # å¦‚æœç›´æ¥æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•é€šè¿‡å…ƒæ•°æ®æŸ¥è¯¢
            all_results = self.collection.get(
                include=['documents', 'metadatas']
            )

            if all_results['documents']:
                for i, (doc, metadata) in enumerate(zip(all_results['documents'], all_results['metadatas'] or [])):
                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ–‡æ¡£ID
                    if metadata.get('id') == doc_id or f"doc_{hash(doc[:100]) % 100000}" == doc_id:
                        return {
                            'id': doc_id,
                            'text': doc,
                            'metadata': metadata or {}
                        }

            return None

        except Exception as e:
            print(f"è·å–æ–‡æ¡£è¯¦æƒ…å¤±è´¥: {e}")
            return None

    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            return {}

        try:
            count = self.collection.count()
            return {
                'total_documents': count,
                'collection_name': self.collection.name,
                'embedding_dimension': settings.EMBEDDING_DIMENSION
            }
        except Exception as e:
            print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def clear_collection(self) -> None:
        """æ¸…ç©ºé›†åˆ"""
        if self.collection:
            try:
                # åˆ é™¤ç°æœ‰é›†åˆ
                self.chroma_client.delete_collection(name="huatuo_medical_qa")
                # é‡æ–°åˆ›å»ºé›†åˆ
                self.collection = self.chroma_client.create_collection(
                    name="huatuo_medical_qa",
                    metadata={"description": "åä½—åŒ»å­¦çŸ¥è¯†å›¾è°±QAå‘é‡é›†åˆ"}
                )
                print("é›†åˆå·²æ¸…ç©º")
            except Exception as e:
                print(f"æ¸…ç©ºé›†åˆå¤±è´¥: {e}")
