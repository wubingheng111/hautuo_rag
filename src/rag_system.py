"""
RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—
"""
from typing import List, Dict, Any, Optional
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.schema import BaseMessage, HumanMessage, AIMessage

from llm_client import DeepSeekLLM, DeepSeekChatClient
from embeddings import EmbeddingManager
from api_embeddings import APIEmbeddingManager
from config import settings

try:
    from langchain_selector import MedicalExampleSelector
    LANGCHAIN_SELECTOR_AVAILABLE = True
except ImportError:
    print("âš ï¸ LangChainç¤ºä¾‹é€‰æ‹©å™¨ä¸å¯ç”¨")
    LANGCHAIN_SELECTOR_AVAILABLE = False

try:
    from cardio_specialist import CardiovascularSpecialist
    CARDIO_SPECIALIST_AVAILABLE = True
except ImportError:
    print("âš ï¸ å¿ƒè¡€ç®¡ä¸“ç§‘æ¨¡å—ä¸å¯ç”¨")
    CARDIO_SPECIALIST_AVAILABLE = False


class MedicalRAGSystem:
    """åŒ»å­¦RAGç³»ç»Ÿ"""

    def __init__(self, use_api_embedding: bool = False, api_provider: str = "huggingface", enable_langchain_selector: bool = True):
        self.llm_client = DeepSeekChatClient()

        # é€‰æ‹©å‘é‡åŒ–æ–¹å¼
        if use_api_embedding:
            print(f"ğŸŒ ä½¿ç”¨APIå‘é‡åŒ–: {api_provider}")
            self.embedding_manager = APIEmbeddingManager(api_provider)
        else:
            print("ğŸ’» ä½¿ç”¨æœ¬åœ°å‘é‡åŒ–")
            self.embedding_manager = EmbeddingManager()

        self.chat_history = ChatMessageHistory()
        self.max_history_length = settings.MAX_HISTORY_LENGTH

        # åˆå§‹åŒ–LangChainç¤ºä¾‹é€‰æ‹©å™¨
        self.example_selector = None
        if enable_langchain_selector and LANGCHAIN_SELECTOR_AVAILABLE:
            try:
                self.example_selector = MedicalExampleSelector()
                print("âœ… LangChainç¤ºä¾‹é€‰æ‹©å™¨å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ LangChainç¤ºä¾‹é€‰æ‹©å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        # åˆå§‹åŒ–å¿ƒè¡€ç®¡ä¸“ç§‘æ¨¡å—
        self.cardio_specialist = None
        if CARDIO_SPECIALIST_AVAILABLE:
            try:
                self.cardio_specialist = CardiovascularSpecialist()
                print("âœ… å¿ƒè¡€ç®¡ä¸“ç§‘æ¨¡å—å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ å¿ƒè¡€ç®¡ä¸“ç§‘æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")

        self._setup_prompts()

    def _setup_prompts(self):
        """è®¾ç½®æç¤ºè¯æ¨¡æ¿"""

        # CoTæ¨ç†æç¤ºè¯
        self.cot_prompt_template = """ä½ æ˜¯åä½—åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·ä½¿ç”¨Chain of Thoughtæ¨ç†æ¥å›ç­”åŒ»å­¦é—®é¢˜ã€‚

## å‚è€ƒèµ„æ–™
{context}

## å¯¹è¯å†å²
{chat_history}

## ç”¨æˆ·é—®é¢˜
{question}

## æ¨ç†è¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œå¹¶åœ¨å›ç­”ä¸­å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

**ç¬¬ä¸€æ­¥ï¼šé—®é¢˜ç†è§£ä¸åˆ†æ**
- åˆ†æç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒè¦ç‚¹
- è¯†åˆ«æ¶‰åŠçš„åŒ»å­¦é¢†åŸŸå’Œå…³é”®æ¦‚å¿µ
- åˆ¤æ–­é—®é¢˜çš„å¤æ‚ç¨‹åº¦å’Œç´§æ€¥ç¨‹åº¦

**ç¬¬äºŒæ­¥ï¼šèµ„æ–™æ£€ç´¢ä¸è¯„ä¼°**
- è¯„ä¼°å‚è€ƒèµ„æ–™çš„ç›¸å…³æ€§å’Œå¯ä¿¡åº¦
- æå–ä¸é—®é¢˜ç›´æ¥ç›¸å…³çš„å…³é”®ä¿¡æ¯
- è¯†åˆ«èµ„æ–™ä¸­çš„é‡è¦åŒ»å­¦äº‹å®å’Œæ•°æ®

**ç¬¬ä¸‰æ­¥ï¼šåŒ»å­¦æ¨ç†**
- åŸºäºåŒ»å­¦çŸ¥è¯†è¿›è¡Œé€»è¾‘æ¨ç†
- è€ƒè™‘å¯èƒ½çš„è¯Šæ–­ã€æ²»ç–—æ–¹æ¡ˆæˆ–è§£é‡Š
- åˆ†æä¸åŒé€‰é¡¹çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨æ€§

**ç¬¬å››æ­¥ï¼šç»¼åˆåˆ¤æ–­**
- ç»“åˆå‚è€ƒèµ„æ–™å’ŒåŒ»å­¦æ¨ç†å¾—å‡ºç»“è®º
- è€ƒè™‘æ‚£è€…å®‰å…¨å’Œæœ€ä½³å®è·µ
- æä¾›ä¸ªæ€§åŒ–çš„å»ºè®®å’Œæ³¨æ„äº‹é¡¹

è¯·åœ¨å›ç­”ä¸­æ¸…æ™°åœ°å±•ç¤ºæ¯ä¸ªæ¨ç†æ­¥éª¤ï¼Œä½¿ç”¨"ğŸ¤” **æ€è€ƒ**ï¼š"æ¥æ ‡è®°ä½ çš„æ¨ç†è¿‡ç¨‹ã€‚

## æˆ‘çš„æ¨ç†å’Œå›ç­”ï¼š"""

        # ç®€åŒ–çš„RAGæç¤ºè¯ï¼ˆç”¨äºéCoTæ¨¡å¼ï¼‰
        self.rag_prompt_template = """ä½ æ˜¯åä½—åŒ»å­¦AIåŠ©æ‰‹ï¼ŒåŸºäºæƒå¨åŒ»å­¦çŸ¥è¯†åº“ä¸ºç”¨æˆ·æä¾›å‡†ç¡®çš„åŒ»å­¦ä¿¡æ¯ã€‚

## å‚è€ƒèµ„æ–™
{context}

## å¯¹è¯å†å²
{chat_history}

## ç”¨æˆ·é—®é¢˜
{question}

è¯·åŸºäºå‚è€ƒèµ„æ–™æä¾›ä¸“ä¸šã€å‡†ç¡®çš„åŒ»å­¦å›ç­”ï¼Œå¹¶æé†’ç”¨æˆ·æœ¬å›ç­”ä»…ä¾›å‚è€ƒï¼Œå…·ä½“è¯Šç–—è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚

## æˆ‘çš„å›ç­”ï¼š"""

        # æ‘˜è¦æç¤ºè¯
        self.summary_prompt_template = """è¯·å¯¹ä»¥ä¸‹åŒ»å­¦æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

åŸæ–‡ï¼š
{text}

è¯·æä¾›ä¸€ä¸ªç®€æ´æ˜äº†çš„æ‘˜è¦ï¼ŒåŒ…å«ï¼š
1. ä¸»è¦åŒ»å­¦æ¦‚å¿µ
2. å…³é”®ä¿¡æ¯ç‚¹
3. é‡è¦çš„è¯Šç–—å»ºè®®

æ‘˜è¦ï¼š"""

        # ä»»åŠ¡é“¾æç¤ºè¯
        self.chain_prompt_template = """ä½œä¸ºåŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†æç”¨æˆ·çš„åŒ»å­¦é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}
å‚è€ƒä¿¡æ¯ï¼š{context}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š
1. é—®é¢˜ç†è§£ï¼šç†è§£ç”¨æˆ·è¯¢é—®çš„åŒ»å­¦é—®é¢˜
2. ä¿¡æ¯æ£€ç´¢ï¼šä»å‚è€ƒä¿¡æ¯ä¸­æå–ç›¸å…³å†…å®¹
3. ä¸“ä¸šåˆ†æï¼šåŸºäºåŒ»å­¦çŸ¥è¯†è¿›è¡Œåˆ†æ
4. å»ºè®®æä¾›ï¼šç»™å‡ºä¸“ä¸šçš„åŒ»å­¦å»ºè®®
5. æ³¨æ„äº‹é¡¹ï¼šæé†’ç›¸å…³æ³¨æ„äº‹é¡¹

åˆ†æç»“æœï¼š"""

    def query(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None,
        compare_mode: bool = False
    ) -> Dict[str, Any]:
        """
        RAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            compare_mode: æ˜¯å¦å¯ç”¨å¯¹æ¯”æ¨¡å¼ï¼ˆæ˜¾ç¤ºæ£€ç´¢å‰åå·®å¼‚ï¼‰

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            # 1. ç”Ÿæˆæ— æ£€ç´¢çš„åŸºç¡€å›ç­”ï¼ˆæ£€ç´¢å‰ï¼‰
            base_answer = ""
            if compare_mode:
                base_answer = self._generate_base_answer(question)

            # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 3. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 4. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 5. æ„å»ºRAGæç¤ºè¯
            prompt = self.rag_prompt_template.format(
                context=context,
                chat_history=chat_history,
                question=question
            )

            # 6. ç”ŸæˆRAGå¢å¼ºå›ç­”ï¼ˆæ£€ç´¢åï¼‰
            rag_answer = self.llm_client.generate_response(
                user_message=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦AIåŠ©æ‰‹ã€‚"
            )

            # 7. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(rag_answer)

            result = {
                'question': question,
                'answer': rag_answer,
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs)
            }

            # 8. å¦‚æœæ˜¯å¯¹æ¯”æ¨¡å¼ï¼Œæ·»åŠ å¯¹æ¯”ä¿¡æ¯
            if compare_mode:
                result.update({
                    'base_answer': base_answer,
                    'rag_answer': rag_answer,
                    'comparison': self._compare_answers(base_answer, rag_answer, similar_docs)
                })

            return result

        except Exception as e:
            print(f"RAGæŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'question': question,
                'answer': f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0
            }

    def _build_context(self, similar_docs: List[Dict[str, Any]]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not similar_docs:
            return "æœªæ‰¾åˆ°ç›¸å…³çš„åŒ»å­¦ä¿¡æ¯ã€‚"

        # è¿‡æ»¤ä½ç›¸å…³æ€§æ–‡æ¡£
        high_quality_docs = [
            doc for doc in similar_docs
            if doc['similarity'] >= settings.SIMILARITY_THRESHOLD
        ]

        if not high_quality_docs:
            return "æœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„åŒ»å­¦ä¿¡æ¯ã€‚"

        context_parts = []
        for i, doc in enumerate(high_quality_docs, 1):
            # æ·»åŠ æ›´ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ–‡æ¡£IDç”¨äºè·³è½¬
            doc_id = doc.get('id', f"doc_{i}")
            source = doc.get('metadata', {}).get('source', 'åä½—åŒ»å­¦çŸ¥è¯†åº“')
            dataset_info = doc.get('metadata', {}).get('dataset_info', '')

            context_parts.append(
                f"ã€å‚è€ƒèµ„æ–™{i}ã€‘ï¼ˆæ–‡æ¡£IDï¼š{doc_id}ï¼Œç›¸å…³åº¦ï¼š{doc['similarity']:.3f}ï¼‰\n"
                f"å†…å®¹ï¼š{doc['text']}\n"
                f"æ¥æºï¼š{source}\n"
                f"æ•°æ®é›†ï¼š{dataset_info}\n"
            )

        return "\n".join(context_parts)

    def get_document_details(self, doc_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        """
        try:
            # ä»å‘é‡æ•°æ®åº“è·å–æ–‡æ¡£è¯¦æƒ…
            doc_details = self.embedding_manager.get_document_by_id(doc_id)

            if doc_details:
                return {
                    'id': doc_id,
                    'content': doc_details.get('text', ''),
                    'metadata': doc_details.get('metadata', {}),
                    'source': doc_details.get('metadata', {}).get('source', 'åä½—åŒ»å­¦çŸ¥è¯†åº“'),
                    'dataset_info': doc_details.get('metadata', {}).get('dataset_info', ''),
                    'question': doc_details.get('metadata', {}).get('question', ''),
                    'answer': doc_details.get('metadata', {}).get('answer', ''),
                    'found': True
                }
            else:
                return {
                    'id': doc_id,
                    'found': False,
                    'error': 'æ–‡æ¡£æœªæ‰¾åˆ°'
                }

        except Exception as e:
            return {
                'id': doc_id,
                'found': False,
                'error': f'è·å–æ–‡æ¡£è¯¦æƒ…å¤±è´¥: {str(e)}'
            }

    def _get_chat_history(self) -> str:
        """è·å–å¯¹è¯å†å²"""
        try:
            messages = self.chat_history.messages
            if not messages:
                return "æ— å¯¹è¯å†å²"

            history_parts = []
            # è·å–æœ€è¿‘çš„å¯¹è¯ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            recent_messages = messages[-(self.max_history_length * 2):]  # æ¯è½®å¯¹è¯åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯

            for msg in recent_messages:
                if isinstance(msg, HumanMessage):
                    role = "ç”¨æˆ·"
                elif isinstance(msg, AIMessage):
                    role = "åŠ©æ‰‹"
                else:
                    role = "ç³»ç»Ÿ"
                history_parts.append(f"{role}: {msg.content}")

            return "\n".join(history_parts)
        except Exception as e:
            print(f"è·å–å¯¹è¯å†å²å¤±è´¥: {e}")
            return "æ— å¯¹è¯å†å²"

    def summarize_text(self, text: str) -> str:
        """
        æ–‡æœ¬æ‘˜è¦åŠŸèƒ½

        Args:
            text: éœ€è¦æ‘˜è¦çš„æ–‡æœ¬

        Returns:
            æ‘˜è¦ç»“æœ
        """
        try:
            prompt = self.summary_prompt_template.format(text=text)
            summary = self.llm_client.generate_response(
                user_message=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦æ–‡æœ¬æ‘˜è¦åŠ©æ‰‹ã€‚"
            )
            return summary
        except Exception as e:
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

    def chain_analysis(self, question: str) -> str:
        """
        ä»»åŠ¡é“¾åˆ†æ

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            åˆ†æç»“æœ
        """
        try:
            # æ£€ç´¢ç›¸å…³ä¿¡æ¯
            similar_docs = self.embedding_manager.search_similar(question)
            context = self._build_context(similar_docs)

            # ä½¿ç”¨ä»»åŠ¡é“¾æç¤ºè¯
            prompt = self.chain_prompt_template.format(
                question=question,
                context=context
            )

            analysis = self.llm_client.generate_response(
                user_message=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åˆ†æåŠ©æ‰‹ï¼Œè¯·æŒ‰æ­¥éª¤è¿›è¡Œåˆ†æã€‚"
            )
            return analysis
        except Exception as e:
            return f"ä»»åŠ¡é“¾åˆ†æå¤±è´¥ï¼š{str(e)}"

    def query_with_cot_reasoning(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨Chain of Thoughtæ¨ç†çš„RAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Returns:
            åŒ…å«æ¨ç†è¿‡ç¨‹çš„æŸ¥è¯¢ç»“æœ
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 3. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 4. ä½¿ç”¨ä¸“é—¨çš„CoTæ¨ç†æç¤º
            cot_reasoning_prompt = f"""ä½ æ˜¯åä½—åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹åŒ»å­¦é—®é¢˜è¿›è¡Œæ·±åº¦Chain of Thoughtæ¨ç†ã€‚

## å‚è€ƒèµ„æ–™
{context}

## ç”¨æˆ·é—®é¢˜
{question}

## æ¨ç†è¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œæ¨ç†ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½è¦è¯¦ç»†å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

ğŸ¤” **ç¬¬ä¸€æ­¥ï¼šé—®é¢˜åˆ†æ**
[è¯¦ç»†åˆ†æç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒè¦ç‚¹ã€æ¶‰åŠçš„åŒ»å­¦é¢†åŸŸ]

ğŸ¤” **ç¬¬äºŒæ­¥ï¼šèµ„æ–™è¯„ä¼°**
[è¯„ä¼°å‚è€ƒèµ„æ–™çš„ç›¸å…³æ€§ï¼Œæå–å…³é”®åŒ»å­¦ä¿¡æ¯]

ğŸ¤” **ç¬¬ä¸‰æ­¥ï¼šåŒ»å­¦æ¨ç†**
[åŸºäºåŒ»å­¦çŸ¥è¯†è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œè€ƒè™‘å¯èƒ½çš„è§£é‡Šæˆ–æ–¹æ¡ˆ]

ğŸ¤” **ç¬¬å››æ­¥ï¼šç»¼åˆåˆ¤æ–­**
[ç»“åˆæ‰€æœ‰ä¿¡æ¯å¾—å‡ºæœ€ç»ˆç»“è®ºå’Œå»ºè®®]

ğŸ’¡ **æœ€ç»ˆå›ç­”**
[åŸºäºæ¨ç†è¿‡ç¨‹ç»™å‡ºä¸“ä¸šçš„åŒ»å­¦å›ç­”]

è¯·å¼€å§‹ä½ çš„æ¨ç†ï¼š"""

            # 5. ç”Ÿæˆæ¨ç†å›ç­”
            reasoning_response = self.llm_client.generate_response(
                user_message=cot_reasoning_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·è¯¦ç»†å±•ç¤ºä½ çš„Chain of Thoughtæ¨ç†è¿‡ç¨‹ã€‚"
            )

            # 6. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(reasoning_response)

            return {
                'question': question,
                'reasoning_response': reasoning_response,
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs),
                'reasoning_type': 'Chain of Thought'
            }

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œæ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            return {
                'question': question,
                'reasoning_response': error_msg,
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0,
                'reasoning_type': 'Error'
            }

    def query_with_langchain_selector(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨LangChainç¤ºä¾‹é€‰æ‹©å™¨çš„RAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Returns:
            åŒ…å«ç¤ºä¾‹é€‰æ‹©è¿‡ç¨‹çš„æŸ¥è¯¢ç»“æœ
        """
        try:
            if not self.example_selector:
                return self.query(question, use_history, top_k)

            # 1. åˆ†æç¤ºä¾‹é€‰æ‹©è¿‡ç¨‹
            selection_analysis = self.example_selector.analyze_selection_process(question)

            # 2. ç”ŸæˆåŒ…å«ç¤ºä¾‹çš„æç¤ºè¯
            few_shot_prompt = self.example_selector.generate_prompt(question)

            # 3. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 4. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 5. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 6. ç»“åˆFew-Shotæç¤ºå’ŒRAGä¸Šä¸‹æ–‡
            enhanced_prompt = f"""
{few_shot_prompt}

å‚è€ƒèµ„æ–™ï¼š
{context}

å¯¹è¯å†å²ï¼š
{chat_history}

è¯·åŸºäºä»¥ä¸Šç¤ºä¾‹å’Œå‚è€ƒèµ„æ–™ï¼Œè¯¦ç»†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
"""

            # 7. ç”Ÿæˆå›ç­”
            answer = self.llm_client.generate_response(
                user_message=enhanced_prompt,
                system_prompt="ä½ æ˜¯åä½—åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·å‚è€ƒç¤ºä¾‹å’Œèµ„æ–™æä¾›ä¸“ä¸šå›ç­”ã€‚"
            )

            # 8. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(answer)

            return {
                'question': question,
                'answer': answer,
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs),
                'selection_analysis': selection_analysis,
                'few_shot_prompt': few_shot_prompt,
                'method': 'LangChainç¤ºä¾‹é€‰æ‹©å™¨ + RAG'
            }

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œä½¿ç”¨ç¤ºä¾‹é€‰æ‹©å™¨æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            return {
                'question': question,
                'answer': error_msg,
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0,
                'selection_analysis': {},
                'few_shot_prompt': "",
                'method': 'Error'
            }

    def analyze_example_selection(self, question: str) -> Dict[str, Any]:
        """
        åˆ†æç¤ºä¾‹é€‰æ‹©è¿‡ç¨‹

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            è¯¦ç»†çš„é€‰æ‹©è¿‡ç¨‹åˆ†æ
        """
        if not self.example_selector:
            return {"error": "ç¤ºä¾‹é€‰æ‹©å™¨æœªå¯ç”¨"}

        return self.example_selector.analyze_selection_process(question)

    def add_custom_example(self, question: str, thinking: str, answer: str):
        """
        æ·»åŠ è‡ªå®šä¹‰ç¤ºä¾‹

        Args:
            question: é—®é¢˜
            thinking: æ€è€ƒè¿‡ç¨‹
            answer: ç­”æ¡ˆ
        """
        if self.example_selector:
            self.example_selector.add_example(question, thinking, answer)
        else:
            print("âš ï¸ ç¤ºä¾‹é€‰æ‹©å™¨æœªå¯ç”¨")

    def query_with_stream(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None
    ):
        """
        æµå¼RAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Yields:
            æµå¼å›å¤çš„æ–‡æœ¬ç‰‡æ®µå’Œå…ƒæ•°æ®
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 3. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 4. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            full_context = ""
            if context:
                full_context += f"å‚è€ƒèµ„æ–™ï¼š\n{context}\n\n"
            if chat_history:
                full_context += f"å¯¹è¯å†å²ï¼š\n{chat_history}\n\n"

            # 5. å…ˆè¿”å›å…ƒæ•°æ®
            yield {
                'type': 'metadata',
                'question': question,
                'doc_count': len(similar_docs),
                'retrieved_docs': similar_docs,
                'context': context
            }

            # 6. æµå¼ç”Ÿæˆå›ç­”
            full_answer = ""
            for chunk in self.llm_client.generate_response_stream(
                user_message=question,
                context=full_context,
                system_prompt="ä½ æ˜¯åä½—åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™è¿›è¡Œä¸“ä¸šçš„åŒ»å­¦å›ç­”ã€‚"
            ):
                full_answer += chunk
                yield {
                    'type': 'content',
                    'chunk': chunk,
                    'full_content': full_answer
                }

            # 7. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(full_answer)

            # 8. è¿”å›å®Œæˆä¿¡å·
            yield {
                'type': 'complete',
                'final_answer': full_answer,
                'method': 'æµå¼RAGæŸ¥è¯¢'
            }

        except Exception as e:
            yield {
                'type': 'error',
                'error': str(e),
                'message': f"æŠ±æ­‰ï¼Œæµå¼æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            }

    def query_cardiovascular_specialist(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None
    ) -> Dict[str, Any]:
        """
        å¿ƒè¡€ç®¡ä¸“ç§‘æŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Returns:
            å¿ƒè¡€ç®¡ä¸“ç§‘æŸ¥è¯¢ç»“æœ
        """
        try:
            if not self.cardio_specialist:
                return self.query(question, use_history, top_k)

            # 1. å¿ƒè¡€ç®¡ç›¸å…³æ€§åˆ†æ
            cardio_analysis = self.cardio_specialist.is_cardiovascular_related(question)

            # 2. æ€¥ç—‡è¯„ä¼°
            emergency_analysis = self.cardio_specialist.assess_emergency_level(question)

            # 3. é£é™©å› ç´ åˆ†æ
            risk_analysis = self.cardio_specialist.analyze_risk_factors(question)

            # 4. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä¼˜å…ˆå¿ƒè¡€ç®¡ç›¸å…³ï¼‰
            cardio_query = question
            if cardio_analysis['is_cardiovascular']:
                # å¢å¼ºæŸ¥è¯¢è¯ï¼Œæé«˜å¿ƒè¡€ç®¡ç›¸å…³æ–‡æ¡£çš„æ£€ç´¢ç²¾åº¦
                top_keywords = [kw for kw_list in cardio_analysis['matched_keywords'].values() for kw in kw_list]
                if top_keywords:
                    cardio_query = f"{question} {' '.join(top_keywords[:3])}"

            similar_docs = self.embedding_manager.search_similar(
                query=cardio_query,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 5. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 6. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 7. ç”Ÿæˆå¿ƒè¡€ç®¡ä¸“ç§‘æç¤ºè¯
            specialist_prompt = self.cardio_specialist.generate_cardio_prompt(question)

            # 8. æ„å»ºå®Œæ•´æç¤º
            full_prompt = f"""
{specialist_prompt}

å‚è€ƒèµ„æ–™ï¼š
{context}

å¯¹è¯å†å²ï¼š
{chat_history}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä½œä¸ºå¿ƒè¡€ç®¡ä¸“ç§‘AIåŠ©æ‰‹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
"""

            # 9. ç”Ÿæˆå›ç­”
            answer = self.llm_client.generate_response(
                user_message=full_prompt,
                system_prompt="ä½ æ˜¯å¿ƒæ™ºåŒ»AIï¼Œä¸“ä¸šçš„å¿ƒè¡€ç®¡ç–¾ç—…æ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚"
            )

            # 10. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(answer)

            return {
                'question': question,
                'answer': answer,
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs),
                'cardio_analysis': cardio_analysis,
                'emergency_analysis': emergency_analysis,
                'risk_analysis': risk_analysis,
                'specialist_prompt': specialist_prompt,
                'method': 'å¿ƒè¡€ç®¡ä¸“ç§‘RAGæŸ¥è¯¢',
                'rag_process': {
                    'original_query': question,
                    'enhanced_query': cardio_query,
                    'retrieval_method': 'semantic_similarity',
                    'knowledge_base': 'huatuo_medical_qa',
                    'embedding_model': 'sentence-transformers',
                    'vector_db': 'chromadb',
                    'retrieval_time': 'real-time',
                    'context_length': len(context),
                    'fusion_method': 'context_injection'
                }
            }

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œå¿ƒè¡€ç®¡ä¸“ç§‘æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            return {
                'question': question,
                'answer': error_msg,
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0,
                'cardio_analysis': {},
                'emergency_analysis': {},
                'risk_analysis': {},
                'specialist_prompt': "",
                'method': 'Error'
            }

    def get_cardio_statistics(self) -> Dict[str, Any]:
        """è·å–å¿ƒè¡€ç®¡ä¸“ç§‘ç»Ÿè®¡ä¿¡æ¯"""
        if self.cardio_specialist:
            return self.cardio_specialist.get_cardio_statistics()
        else:
            return {"error": "å¿ƒè¡€ç®¡ä¸“ç§‘æ¨¡å—æœªå¯ç”¨"}

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history.clear()

    def _preprocess_question(self, question: str) -> str:
        """é¢„å¤„ç†ç”¨æˆ·é—®é¢˜"""
        # å»é™¤å¤šä½™ç©ºæ ¼
        question = question.strip()

        # æ·»åŠ åŒ»å­¦ç›¸å…³å…³é”®è¯æå–
        medical_keywords = ['ç—‡çŠ¶', 'ç–¾ç—…', 'æ²»ç–—', 'è¯ç‰©', 'è¯Šæ–­', 'ç—…å› ', 'é¢„é˜²']

        # å¦‚æœé—®é¢˜å¤ªçŸ­ï¼Œå»ºè®®ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯
        if len(question) < 5:
            return question + "ï¼ˆå»ºè®®æä¾›æ›´è¯¦ç»†çš„æè¿°ä»¥è·å¾—æ›´å‡†ç¡®çš„å›ç­”ï¼‰"

        return question

    def _postprocess_answer(self, answer: str, retrieved_docs: List[Dict[str, Any]]) -> str:
        """åå¤„ç†å›ç­”"""
        # ç¡®ä¿å›ç­”åŒ…å«å®‰å…¨æé†’
        safety_reminder = "\n\nâš ï¸ **é‡è¦æé†’**ï¼šæœ¬å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰å¥åº·é—®é¢˜ï¼Œè¯·åŠæ—¶å°±åŒ»å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚"

        if "ä»…ä¾›å‚è€ƒ" not in answer and "å’¨è¯¢åŒ»ç”Ÿ" not in answer:
            answer += safety_reminder

        # å¦‚æœæœ‰å‚è€ƒèµ„æ–™ï¼Œæ·»åŠ èµ„æ–™æ¥æºè¯´æ˜
        if retrieved_docs:
            answer += f"\n\nğŸ“š **å‚è€ƒèµ„æ–™**ï¼šåŸºäº{len(retrieved_docs)}æ¡åä½—åŒ»å­¦çŸ¥è¯†åº“èµ„æ–™"

        return answer

    def query_with_native_cot(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨DeepSeekåŸç”ŸCoTæ¨ç†çš„RAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Returns:
            åŒ…å«åŸç”ŸCoTæ¨ç†è¿‡ç¨‹çš„æŸ¥è¯¢ç»“æœ
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 3. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 4. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            full_context = ""
            if context:
                full_context += f"å‚è€ƒèµ„æ–™ï¼š\n{context}\n\n"
            if chat_history:
                full_context += f"å¯¹è¯å†å²ï¼š\n{chat_history}\n\n"

            # 5. ä½¿ç”¨DeepSeekåŸç”ŸCoTæ¨ç†
            cot_result = self.llm_client.generate_response_with_cot(
                user_message=question,
                context=full_context,
                system_prompt="ä½ æ˜¯åä½—åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™è¿›è¡Œä¸“ä¸šçš„åŒ»å­¦å›ç­”ã€‚"
            )

            # 6. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(cot_result['final_answer'])

            return {
                'question': question,
                'thinking_process': cot_result['thinking_process'],
                'answer': cot_result['final_answer'],
                'full_response': cot_result['full_response'],
                'has_explicit_cot': cot_result['has_explicit_cot'],
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs),
                'method': 'DeepSeekåŸç”ŸCoTæ¨ç†'
            }

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼ŒCoTæ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            return {
                'question': question,
                'thinking_process': f"æ¨ç†è¿‡ç¨‹å‡ºç°é”™è¯¯ï¼š{str(e)}",
                'answer': error_msg,
                'full_response': error_msg,
                'has_explicit_cot': False,
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0,
                'method': 'Error'
            }

    def query_with_thinking_process(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None,
        show_thinking: bool = True
    ) -> Dict[str, Any]:
        """
        å¸¦æ€è€ƒè¿‡ç¨‹çš„RAGæŸ¥è¯¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹

        Returns:
            åŒ…å«æ€è€ƒè¿‡ç¨‹çš„æŸ¥è¯¢ç»“æœ
        """
        thinking_steps = []

        try:
            # æ­¥éª¤1: é—®é¢˜åˆ†æ
            if show_thinking:
                thinking_steps.append("ğŸ¤” æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")

            # æ­¥éª¤2: çŸ¥è¯†æ£€ç´¢
            if show_thinking:
                thinking_steps.append("ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³åŒ»å­¦çŸ¥è¯†...")

            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            if show_thinking:
                thinking_steps.append(f"âœ… æ‰¾åˆ° {len(similar_docs)} æ¡ç›¸å…³èµ„æ–™")

            # æ­¥éª¤3: æ„å»ºä¸Šä¸‹æ–‡
            if show_thinking:
                thinking_steps.append("ğŸ”— æ­£åœ¨æ•´ç†å‚è€ƒä¿¡æ¯...")

            context = self._build_context(similar_docs)

            # æ­¥éª¤4: è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                if show_thinking:
                    thinking_steps.append("ğŸ’­ æ­£åœ¨å›é¡¾å¯¹è¯å†å²...")
                chat_history = self._get_chat_history()

            # æ­¥éª¤5: CoTæ¨ç†ç”Ÿæˆå›ç­”
            if show_thinking:
                thinking_steps.append("ğŸ§  æ­£åœ¨è¿›è¡ŒChain of Thoughtæ¨ç†...")

            # ä½¿ç”¨CoTæ¨ç†æç¤ºè¯
            cot_prompt = self.cot_prompt_template.format(
                context=context,
                chat_history=chat_history,
                question=question
            )

            # ç”ŸæˆåŒ…å«æ¨ç†è¿‡ç¨‹çš„å›ç­”
            answer = self.llm_client.generate_response(
                user_message=cot_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·å±•ç¤ºä½ çš„æ¨ç†æ€è€ƒè¿‡ç¨‹ã€‚"
            )

            if show_thinking:
                thinking_steps.append("âœ¨ å›ç­”ç”Ÿæˆå®Œæˆï¼")

            # æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(answer)

            return {
                'question': question,
                'answer': answer,
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs),
                'thinking_steps': thinking_steps
            }

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            thinking_steps.append(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

            return {
                'question': question,
                'answer': error_msg,
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0,
                'thinking_steps': thinking_steps
            }

    def query_with_comparison(
        self,
        question: str,
        use_history: bool = True,
        top_k: int = None
    ) -> Dict[str, Any]:
        """
        å¸¦å¯¹æ¯”çš„RAGæŸ¥è¯¢ - å±•ç¤ºæ£€ç´¢å‰åçš„å·®å¼‚

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        Returns:
            åŒ…å«å¯¹æ¯”ä¿¡æ¯çš„æŸ¥è¯¢ç»“æœ
        """
        try:
            # 1. ç”Ÿæˆæ— æ£€ç´¢çš„åŸºç¡€å›ç­”ï¼ˆæ£€ç´¢å‰ï¼‰
            print("ğŸ” æ­£åœ¨ç”ŸæˆåŸºç¡€å›ç­”ï¼ˆæ— çŸ¥è¯†æ£€ç´¢ï¼‰...")
            base_prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·ä»…åŸºäºä½ çš„åŸºç¡€çŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•å¤–éƒ¨å‚è€ƒèµ„æ–™ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›ä½ çš„å›ç­”ï¼š"""

            base_answer = self.llm_client.generate_response(
                user_message=base_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦AIåŠ©æ‰‹ï¼Œä»…ä½¿ç”¨åŸºç¡€åŒ»å­¦çŸ¥è¯†å›ç­”é—®é¢˜ã€‚"
            )

            # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            print("ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³åŒ»å­¦çŸ¥è¯†...")
            similar_docs = self.embedding_manager.search_similar(
                query=question,
                top_k=top_k or settings.TOP_K_RETRIEVAL
            )

            # 3. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(similar_docs)

            # 4. è·å–å¯¹è¯å†å²
            chat_history = ""
            if use_history:
                chat_history = self._get_chat_history()

            # 5. æ„å»ºRAGæç¤ºè¯
            rag_prompt = self.rag_prompt_template.format(
                context=context,
                chat_history=chat_history,
                question=question
            )

            # 6. ç”ŸæˆRAGå¢å¼ºå›ç­”ï¼ˆæ£€ç´¢åï¼‰
            print("ğŸ§  æ­£åœ¨ç”ŸæˆRAGå¢å¼ºå›ç­”ï¼ˆåŸºäºæ£€ç´¢çŸ¥è¯†ï¼‰...")
            rag_answer = self.llm_client.generate_response(
                user_message=rag_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦AIåŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ã€‚"
            )

            # 7. ç”Ÿæˆå¯¹æ¯”åˆ†æ
            comparison_analysis = self._generate_comparison_analysis(
                question, base_answer, rag_answer, similar_docs
            )

            # 8. æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self.chat_history.add_user_message(question)
                self.chat_history.add_ai_message(rag_answer)

            return {
                'question': question,
                'base_answer': base_answer,
                'rag_answer': rag_answer,
                'final_answer': rag_answer,  # æœ€ç»ˆé‡‡ç”¨RAGç­”æ¡ˆ
                'context': context,
                'retrieved_docs': similar_docs,
                'doc_count': len(similar_docs),
                'comparison_analysis': comparison_analysis,
                'improvement_summary': self._summarize_improvements(base_answer, rag_answer, similar_docs)
            }

        except Exception as e:
            print(f"RAGå¯¹æ¯”æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'question': question,
                'base_answer': f"åŸºç¡€å›ç­”ç”Ÿæˆå¤±è´¥ï¼š{str(e)}",
                'rag_answer': f"RAGå›ç­”ç”Ÿæˆå¤±è´¥ï¼š{str(e)}",
                'final_answer': f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                'context': "",
                'retrieved_docs': [],
                'doc_count': 0,
                'comparison_analysis': "å¯¹æ¯”åˆ†æå¤±è´¥",
                'improvement_summary': "æ”¹è¿›æ€»ç»“å¤±è´¥"
            }

    def _generate_comparison_analysis(
        self,
        question: str,
        base_answer: str,
        rag_answer: str,
        retrieved_docs: List[Dict[str, Any]]
    ) -> str:
        """ç”Ÿæˆå¯¹æ¯”åˆ†æ"""

        analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªåŒ»å­¦å›ç­”çš„å·®å¼‚å’Œæ”¹è¿›ï¼š

åŸå§‹é—®é¢˜ï¼š{question}

åŸºç¡€å›ç­”ï¼ˆæ— çŸ¥è¯†æ£€ç´¢ï¼‰ï¼š
{base_answer}

RAGå¢å¼ºå›ç­”ï¼ˆåŸºäºæ£€ç´¢çŸ¥è¯†ï¼‰ï¼š
{rag_answer}

æ£€ç´¢åˆ°çš„å‚è€ƒæ–‡æ¡£æ•°é‡ï¼š{len(retrieved_docs)}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œå¯¹æ¯”åˆ†æï¼š
1. å‡†ç¡®æ€§ï¼šå“ªä¸ªå›ç­”æ›´å‡†ç¡®ï¼Ÿ
2. å®Œæ•´æ€§ï¼šå“ªä¸ªå›ç­”æ›´å…¨é¢ï¼Ÿ
3. ä¸“ä¸šæ€§ï¼šå“ªä¸ªå›ç­”æ›´ä¸“ä¸šï¼Ÿ
4. å…·ä½“æ€§ï¼šå“ªä¸ªå›ç­”æä¾›äº†æ›´å…·ä½“çš„ä¿¡æ¯ï¼Ÿ
5. å¯ä¿¡åº¦ï¼šå“ªä¸ªå›ç­”æ›´å¯ä¿¡ï¼Ÿ

åˆ†æç»“æœï¼š"""

        try:
            analysis = self.llm_client.generate_response(
                user_message=analysis_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œè¯·å®¢è§‚åˆ†æä¸¤ä¸ªå›ç­”çš„å·®å¼‚ã€‚"
            )
            return analysis
        except Exception as e:
            return f"å¯¹æ¯”åˆ†æç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

    def _summarize_improvements(
        self,
        base_answer: str,
        rag_answer: str,
        retrieved_docs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ€»ç»“RAGå¸¦æ¥çš„æ”¹è¿›"""

        improvements = {
            'knowledge_enhancement': len(retrieved_docs) > 0,
            'doc_count': len(retrieved_docs),
            'answer_length_change': len(rag_answer) - len(base_answer),
            'has_references': len(retrieved_docs) > 0,
            'confidence_boost': len(retrieved_docs) > 0
        }

        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        base_words = set(base_answer.split())
        rag_words = set(rag_answer.split())

        improvements['content_overlap'] = len(base_words.intersection(rag_words)) / len(base_words.union(rag_words)) if base_words.union(rag_words) else 0
        improvements['new_content_ratio'] = len(rag_words - base_words) / len(rag_words) if rag_words else 0

        return improvements

    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        embedding_stats = self.embedding_manager.get_collection_stats()

        return {
            'vector_db_stats': embedding_stats,
            'memory_length': len(self.chat_history.messages),
            'model_info': {
                'llm_model': settings.DEEPSEEK_MODEL,
                'embedding_model': settings.EMBEDDING_MODEL
            }
        }
