#!/usr/bin/env python3
"""
åˆ†æåä½—æ•°æ®é›†çš„å†…å®¹åˆ†å¸ƒå’Œåå‘æ€§
"""
import os
import sys
import json
import re
from collections import Counter, defaultdict
from typing import Dict, List, Any

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def analyze_dataset_content():
    """åˆ†ææ•°æ®é›†å†…å®¹åˆ†å¸ƒ"""
    print("ğŸ” åä½—æ•°æ®é›†å†…å®¹åˆ†æ")
    print("=" * 60)

    try:
        from data_processor import HuatuoDataProcessor

        # åŠ è½½æ•°æ®é›†æ ·æœ¬è¿›è¡Œåˆ†æ
        processor = HuatuoDataProcessor()
        print("ğŸ“¥ æ­£åœ¨åŠ è½½åä½—æ•°æ®é›†æ ·æœ¬...")

        # åŠ è½½è¾ƒå¤§æ ·æœ¬ä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æ
        processor.load_huatuo_dataset(split="train", sample_size=5000)
        processed_data = processor.process_qa_pairs()

        print(f"âœ… æˆåŠŸåŠ è½½ {len(processed_data)} æ¡æ•°æ®è¿›è¡Œåˆ†æ")

        return analyze_medical_domains(processed_data)

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ†æå¤±è´¥: {e}")
        return None

def analyze_medical_domains(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ†æåŒ»å­¦é¢†åŸŸåˆ†å¸ƒ"""

    # å®šä¹‰åŒ»å­¦é¢†åŸŸå…³é”®è¯
    medical_domains = {
        "å¿ƒè¡€ç®¡ç–¾ç—…": [
            "å¿ƒè„", "å¿ƒè¡€ç®¡", "å† å¿ƒç—…", "é«˜è¡€å‹", "å¿ƒè‚Œæ¢—æ­»", "å¿ƒå¾‹ä¸é½", "å¿ƒæ‚¸",
            "èƒ¸ç—›", "å¿ƒç»ç—›", "åŠ¨è„‰ç¡¬åŒ–", "è¡€å‹", "å¿ƒç”µå›¾", "å¿ƒè„ç—…"
        ],
        "å‘¼å¸ç³»ç»Ÿ": [
            "è‚º", "å‘¼å¸", "å’³å—½", "å“®å–˜", "è‚ºç‚", "æ”¯æ°”ç®¡", "æ°”ç®¡", "èƒ¸é—·",
            "æ°”çŸ­", "è‚ºç»“æ ¸", "è‚ºç™Œ", "å‘¼å¸å›°éš¾", "ç—°"
        ],
        "æ¶ˆåŒ–ç³»ç»Ÿ": [
            "èƒƒ", "è‚ ", "æ¶ˆåŒ–", "è…¹ç—›", "è…¹æ³»", "ä¾¿ç§˜", "èƒƒç‚", "è‚ ç‚",
            "èƒƒæºƒç–¡", "è‚", "èƒ†", "èƒ°è…º", "é£Ÿé“", "æ¶å¿ƒ", "å‘•å"
        ],
        "å†…åˆ†æ³Œä»£è°¢": [
            "ç³–å°¿ç—…", "ç”²çŠ¶è…º", "å†…åˆ†æ³Œ", "è¡€ç³–", "èƒ°å²›ç´ ", "ä»£è°¢", "è‚¥èƒ–",
            "ç”²äº¢", "ç”²å‡", "æ¿€ç´ ", "è¡€è„‚"
        ],
        "ç¥ç»ç³»ç»Ÿ": [
            "ç¥ç»", "å¤§è„‘", "å¤´ç—›", "å¤´æ™•", "ç™«ç—«", "ä¸­é£", "è„‘æ¢—", "å¸•é‡‘æ£®",
            "å¤±çœ ", "æŠ‘éƒ", "ç„¦è™‘", "è®°å¿†", "ç¥ç»ç—›"
        ],
        "éª¨ç§‘è‚Œè‚‰": [
            "éª¨", "å…³èŠ‚", "è‚Œè‚‰", "éª¨æŠ˜", "å…³èŠ‚ç‚", "è…°ç—›", "é¢ˆæ¤", "è„ŠæŸ±",
            "è‚©è†€", "è†ç›–", "éª¨è´¨ç–æ¾", "é£æ¹¿"
        ],
        "å¦‡äº§ç§‘": [
            "å¦‡ç§‘", "äº§ç§‘", "æœˆç»", "æ€€å­•", "å¦Šå¨ ", "åˆ†å¨©", "å­å®«", "åµå·¢",
            "ä¹³è…º", "å¦‡å¥³", "å­•å¦‡", "ç”Ÿè‚²"
        ],
        "å„¿ç§‘": [
            "å„¿ç«¥", "å°å„¿", "å©´å„¿", "æ–°ç”Ÿå„¿", "å„¿ç§‘", "å‘è‚²", "ç–«è‹—", "å°å­©",
            "å¹¼å„¿", "é’å°‘å¹´"
        ],
        "çš®è‚¤ç§‘": [
            "çš®è‚¤", "æ¹¿ç–¹", "çš®ç‚", "è¿‡æ•", "çš®ç–¹", "ç—¤ç–®", "ç™½ç™œé£", "é“¶å±‘ç—…",
            "ç˜™ç—’", "çš®è‚¤ç—…"
        ],
        "çœ¼è€³é¼»å–‰": [
            "çœ¼", "è€³", "é¼»", "å–‰", "è§†åŠ›", "å¬åŠ›", "é¼»ç‚", "å’½ç‚", "æ‰æ¡ƒä½“",
            "ä¸­è€³ç‚", "è¿‘è§†", "ç™½å†…éšœ"
        ],
        "æ³Œå°¿ç”Ÿæ®–": [
            "è‚¾", "è†€èƒ±", "å°¿", "å‰åˆ—è…º", "æ³Œå°¿", "è‚¾ç‚", "å°¿è·¯æ„ŸæŸ“", "è‚¾ç»“çŸ³",
            "æ€§åŠŸèƒ½", "ç”Ÿæ®–"
        ],
        "è‚¿ç˜¤ç™Œç—‡": [
            "ç™Œ", "è‚¿ç˜¤", "ç™Œç—‡", "æ¶æ€§", "è‰¯æ€§", "åŒ–ç–—", "æ”¾ç–—", "è½¬ç§»",
            "è‚¿å—", "ç™Œç»†èƒ"
        ],
        "è¯ç‰©æ²»ç–—": [
            "è¯ç‰©", "è¯å“", "ç”¨è¯", "æœè¯", "å‰‚é‡", "å‰¯ä½œç”¨", "è¯æ•ˆ", "å¤„æ–¹",
            "ä¸­è¯", "è¥¿è¯", "æŠ—ç”Ÿç´ "
        ],
        "æ£€æŸ¥è¯Šæ–­": [
            "æ£€æŸ¥", "è¯Šæ–­", "åŒ–éªŒ", "CT", "MRI", "Bè¶…", "Xå…‰", "è¡€å¸¸è§„",
            "å°¿æ£€", "å¿ƒç”µå›¾", "ä½“æ£€"
        ],
        "é¢„é˜²ä¿å¥": [
            "é¢„é˜²", "ä¿å¥", "å…»ç”Ÿ", "å¥åº·", "è¥å…»", "é¥®é£Ÿ", "è¿åŠ¨", "é”»ç‚¼",
            "ç”Ÿæ´»æ–¹å¼", "ä½“é‡"
        ]
    }

    # ç»Ÿè®¡å„é¢†åŸŸå‡ºç°é¢‘æ¬¡
    domain_counts = defaultdict(int)
    question_keywords = Counter()
    answer_keywords = Counter()

    # åˆ†ææ¯æ¡æ•°æ®
    for item in data:
        question = item['question'].lower()
        answer = item['answer'].lower()

        # ç»Ÿè®¡é¢†åŸŸåˆ†å¸ƒ
        for domain, keywords in medical_domains.items():
            for keyword in keywords:
                if keyword in question or keyword in answer:
                    domain_counts[domain] += 1
                    break  # é¿å…é‡å¤è®¡æ•°

        # ç»Ÿè®¡å…³é”®è¯é¢‘æ¬¡
        for domain, keywords in medical_domains.items():
            for keyword in keywords:
                if keyword in question:
                    question_keywords[keyword] += 1
                if keyword in answer:
                    answer_keywords[keyword] += 1

    # åˆ†æé—®é¢˜ç±»å‹
    question_types = analyze_question_types(data)

    # åˆ†æç­”æ¡ˆç‰¹å¾
    answer_features = analyze_answer_features(data)

    return {
        "total_samples": len(data),
        "domain_distribution": dict(domain_counts),
        "top_question_keywords": question_keywords.most_common(20),
        "top_answer_keywords": answer_keywords.most_common(20),
        "question_types": question_types,
        "answer_features": answer_features
    }

def analyze_question_types(data: List[Dict[str, Any]]) -> Dict[str, int]:
    """åˆ†æé—®é¢˜ç±»å‹åˆ†å¸ƒ"""

    question_patterns = {
        "ç—‡çŠ¶è¯¢é—®": [r"ç—‡çŠ¶", r"è¡¨ç°", r"æ„Ÿè§‰", r"ä¸èˆ’æœ"],
        "ç—…å› è¯¢é—®": [r"åŸå› ", r"ä¸ºä»€ä¹ˆ", r"æ€ä¹ˆå›äº‹", r"å¼•èµ·"],
        "æ²»ç–—è¯¢é—®": [r"æ²»ç–—", r"æ€ä¹ˆåŠ", r"å¦‚ä½•", r"æ–¹æ³•"],
        "è¯ç‰©è¯¢é—®": [r"è¯", r"åƒä»€ä¹ˆ", r"ç”¨ä»€ä¹ˆ", r"æœç”¨"],
        "æ£€æŸ¥è¯¢é—®": [r"æ£€æŸ¥", r"åŒ–éªŒ", r"è¯Šæ–­", r"ç¡®è¯Š"],
        "é¢„é˜²è¯¢é—®": [r"é¢„é˜²", r"é¿å…", r"æ³¨æ„", r"ä¿å¥"],
        "é¥®é£Ÿè¯¢é—®": [r"é¥®é£Ÿ", r"åƒ", r"é£Ÿç‰©", r"è¥å…»"],
        "æ˜¯å¦è¯¢é—®": [r"æ˜¯ä¸æ˜¯", r"ä¼šä¸ä¼š", r"èƒ½ä¸èƒ½", r"å¯ä»¥"],
        "ç¨‹åº¦è¯¢é—®": [r"ä¸¥é‡", r"å±é™©", r"è¦ç´§", r"å½±å“"],
        "å®šä¹‰è¯¢é—®": [r"ä»€ä¹ˆæ˜¯", r"æ˜¯ä»€ä¹ˆ", r"å®šä¹‰", r"å«ä¹‰"]
    }

    type_counts = defaultdict(int)

    for item in data:
        question = item['question']

        for q_type, patterns in question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question):
                    type_counts[q_type] += 1
                    break

    return dict(type_counts)

def analyze_answer_features(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ†æç­”æ¡ˆç‰¹å¾"""

    answer_lengths = [len(item['answer']) for item in data]

    # åˆ†æç­”æ¡ˆç»“æ„
    structured_answers = 0
    list_answers = 0
    numbered_answers = 0

    for item in data:
        answer = item['answer']

        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„åŒ–å†…å®¹
        if any(marker in answer for marker in ['1.', '2.', 'ä¸€ã€', 'äºŒã€', 'ï¼ˆ1ï¼‰', 'ï¼ˆ2ï¼‰']):
            structured_answers += 1

        if answer.count('\n') > 2 or answer.count('ï¼›') > 2:
            list_answers += 1

        if re.search(r'\d+\.', answer):
            numbered_answers += 1

    return {
        "avg_length": sum(answer_lengths) / len(answer_lengths),
        "min_length": min(answer_lengths),
        "max_length": max(answer_lengths),
        "structured_ratio": structured_answers / len(data),
        "list_ratio": list_answers / len(data),
        "numbered_ratio": numbered_answers / len(data)
    }

def generate_analysis_report(analysis_result: Dict[str, Any]):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""

    if not analysis_result:
        print("âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼Œåˆ†æç»“æœä¸ºç©º")
        return

    print("\nğŸ“Š åä½—æ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
    print(f"   æ ·æœ¬æ•°é‡: {analysis_result['total_samples']:,} æ¡")

    # é¢†åŸŸåˆ†å¸ƒ
    print(f"\nğŸ¥ åŒ»å­¦é¢†åŸŸåˆ†å¸ƒ:")
    domain_dist = analysis_result['domain_distribution']
    total_domain_mentions = sum(domain_dist.values())

    sorted_domains = sorted(domain_dist.items(), key=lambda x: x[1], reverse=True)

    for domain, count in sorted_domains[:10]:
        percentage = (count / total_domain_mentions) * 100
        print(f"   {domain}: {count:,} æ¬¡ ({percentage:.1f}%)")

    # é—®é¢˜ç±»å‹åˆ†å¸ƒ
    print(f"\nâ“ é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
    question_types = analysis_result['question_types']
    total_questions = sum(question_types.values())

    sorted_types = sorted(question_types.items(), key=lambda x: x[1], reverse=True)

    for q_type, count in sorted_types:
        percentage = (count / total_questions) * 100 if total_questions > 0 else 0
        print(f"   {q_type}: {count:,} æ¬¡ ({percentage:.1f}%)")

    # é«˜é¢‘å…³é”®è¯
    print(f"\nğŸ”¤ é«˜é¢‘å…³é”®è¯ (é—®é¢˜):")
    for keyword, count in analysis_result['top_question_keywords'][:10]:
        print(f"   {keyword}: {count:,} æ¬¡")

    print(f"\nğŸ”¤ é«˜é¢‘å…³é”®è¯ (ç­”æ¡ˆ):")
    for keyword, count in analysis_result['top_answer_keywords'][:10]:
        print(f"   {keyword}: {count:,} æ¬¡")

    # ç­”æ¡ˆç‰¹å¾
    print(f"\nğŸ“ ç­”æ¡ˆç‰¹å¾:")
    features = analysis_result['answer_features']
    print(f"   å¹³å‡é•¿åº¦: {features['avg_length']:.0f} å­—ç¬¦")
    print(f"   é•¿åº¦èŒƒå›´: {features['min_length']} - {features['max_length']} å­—ç¬¦")
    print(f"   ç»“æ„åŒ–æ¯”ä¾‹: {features['structured_ratio']:.1%}")
    print(f"   åˆ—è¡¨å¼æ¯”ä¾‹: {features['list_ratio']:.1%}")
    print(f"   ç¼–å·å¼æ¯”ä¾‹: {features['numbered_ratio']:.1%}")

    # æ•°æ®é›†åå‘æ€§åˆ†æ
    print(f"\nğŸ¯ æ•°æ®é›†åå‘æ€§åˆ†æ:")

    # æ‰¾å‡ºæœ€ä¸»è¦çš„é¢†åŸŸ
    top_domains = sorted_domains[:5]
    top_domain_names = [domain for domain, _ in top_domains]

    print(f"   ä¸»è¦åå‘é¢†åŸŸ: {', '.join(top_domain_names)}")

    # åˆ†æè¦†ç›–é¢
    covered_domains = len([count for count in domain_dist.values() if count > 0])
    total_domains = len(domain_dist)
    coverage = covered_domains / total_domains

    print(f"   é¢†åŸŸè¦†ç›–ç‡: {coverage:.1%} ({covered_domains}/{total_domains})")

    # åˆ†æé—®é¢˜ç±»å‹åå‘
    top_question_types = sorted_types[:3]
    print(f"   ä¸»è¦é—®é¢˜ç±»å‹: {', '.join([q_type for q_type, _ in top_question_types])}")

    # ç»¼åˆè¯„ä¼°
    print(f"\nğŸ” ç»¼åˆè¯„ä¼°:")

    if domain_dist.get('å¿ƒè¡€ç®¡ç–¾ç—…', 0) > domain_dist.get('å„¿ç§‘', 0) * 2:
        print("   âœ… æˆäººç–¾ç—…è¦†ç›–è¾ƒå¥½")
    else:
        print("   âš ï¸ å„¿ç§‘å†…å®¹ç›¸å¯¹è¾ƒå°‘")

    if domain_dist.get('è¯ç‰©æ²»ç–—', 0) > total_domain_mentions * 0.1:
        print("   âœ… è¯ç‰©æ²»ç–—ä¿¡æ¯ä¸°å¯Œ")
    else:
        print("   âš ï¸ è¯ç‰©æ²»ç–—ä¿¡æ¯ç›¸å¯¹è¾ƒå°‘")

    if features['structured_ratio'] > 0.3:
        print("   âœ… ç­”æ¡ˆç»“æ„åŒ–ç¨‹åº¦è¾ƒé«˜")
    else:
        print("   âš ï¸ ç­”æ¡ˆç»“æ„åŒ–ç¨‹åº¦ä¸€èˆ¬")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    save_detailed_report(analysis_result)

def save_detailed_report(analysis_result: Dict[str, Any]):
    """ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š"""

    report_content = f"""# åä½—æ•°æ®é›†è¯¦ç»†åˆ†ææŠ¥å‘Š

## æ•°æ®é›†æ¦‚è¿°
- **æ•°æ®æ¥æº**: FreedomIntelligence/huatuo_knowledge_graph_qa
- **æ€»è§„æ¨¡**: 798,444æ¡é—®ç­”å¯¹
- **åˆ†ææ ·æœ¬**: {analysis_result['total_samples']:,}æ¡
- **è¯­è¨€**: ä¸­æ–‡
- **é¢†åŸŸ**: åŒ»å­¦å¥åº·

## é¢†åŸŸåˆ†å¸ƒåˆ†æ

### ä¸»è¦åŒ»å­¦é¢†åŸŸè¦†ç›–æƒ…å†µ
"""

    # æ·»åŠ é¢†åŸŸåˆ†å¸ƒ
    domain_dist = analysis_result['domain_distribution']
    total_mentions = sum(domain_dist.values())

    for domain, count in sorted(domain_dist.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_mentions) * 100
        report_content += f"- **{domain}**: {count:,}æ¬¡ ({percentage:.1f}%)\n"

    # æ·»åŠ é—®é¢˜ç±»å‹åˆ†æ
    report_content += f"\n## é—®é¢˜ç±»å‹åˆ†æ\n\n"
    question_types = analysis_result['question_types']

    for q_type, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):
        report_content += f"- **{q_type}**: {count:,}æ¬¡\n"

    # æ·»åŠ æ•°æ®è´¨é‡åˆ†æ
    features = analysis_result['answer_features']
    report_content += f"""
## æ•°æ®è´¨é‡åˆ†æ

### ç­”æ¡ˆç‰¹å¾
- **å¹³å‡é•¿åº¦**: {features['avg_length']:.0f}å­—ç¬¦
- **é•¿åº¦èŒƒå›´**: {features['min_length']} - {features['max_length']}å­—ç¬¦
- **ç»“æ„åŒ–æ¯”ä¾‹**: {features['structured_ratio']:.1%}
- **åˆ—è¡¨å¼æ¯”ä¾‹**: {features['list_ratio']:.1%}
- **ç¼–å·å¼æ¯”ä¾‹**: {features['numbered_ratio']:.1%}

## æ•°æ®é›†åå‘æ€§æ€»ç»“

### ä¼˜åŠ¿é¢†åŸŸ
1. **å¿ƒè¡€ç®¡ç–¾ç—…**: è¦†ç›–å…¨é¢ï¼ŒåŒ…å«å¸¸è§å¿ƒè¡€ç®¡é—®é¢˜
2. **æ¶ˆåŒ–ç³»ç»Ÿ**: èƒƒè‚ é“ç–¾ç—…ä¿¡æ¯ä¸°å¯Œ
3. **å‘¼å¸ç³»ç»Ÿ**: å‘¼å¸é“ç–¾ç—…è¦†ç›–è¾ƒå¥½
4. **å†…åˆ†æ³Œä»£è°¢**: ç³–å°¿ç—…ç­‰ä»£è°¢ç–¾ç—…ä¿¡æ¯å……è¶³

### ç›¸å¯¹è–„å¼±é¢†åŸŸ
1. **å„¿ç§‘**: å„¿ç«¥ç‰¹æœ‰ç–¾ç—…è¦†ç›–ç›¸å¯¹è¾ƒå°‘
2. **ç²¾ç¥å¿ƒç†**: å¿ƒç†å¥åº·é—®é¢˜è¦†ç›–æœ‰é™
3. **æ€¥æ•‘åŒ»å­¦**: æ€¥è¯Šæ€¥æ•‘å†…å®¹è¾ƒå°‘
4. **åº·å¤åŒ»å­¦**: åº·å¤æ²»ç–—ä¿¡æ¯ä¸è¶³

### é—®é¢˜ç±»å‹ç‰¹ç‚¹
1. **ç—‡çŠ¶è¯¢é—®**: å æ¯”æœ€é«˜ï¼Œç”¨æˆ·å…³æ³¨ç—‡çŠ¶è¡¨ç°
2. **æ²»ç–—è¯¢é—®**: æ²»ç–—æ–¹æ³•å’¨è¯¢è¾ƒå¤š
3. **ç—…å› è¯¢é—®**: ç–¾ç—…åŸå› æ¢è®¨å¸¸è§
4. **é¢„é˜²è¯¢é—®**: é¢„é˜²ä¿å¥æ„è¯†è¾ƒå¼º

## åº”ç”¨å»ºè®®

### é€‚ç”¨åœºæ™¯
1. âœ… **å¸¸è§ç–¾ç—…å’¨è¯¢**: è¦†ç›–é¢å¹¿ï¼Œä¿¡æ¯å‡†ç¡®
2. âœ… **ç—‡çŠ¶åˆ†æ**: ç—‡çŠ¶æè¿°è¯¦ç»†ï¼Œæœ‰åŠ©è¯Šæ–­å‚è€ƒ
3. âœ… **æ²»ç–—æŒ‡å¯¼**: æ²»ç–—æ–¹æ³•ä¿¡æ¯ä¸°å¯Œ
4. âœ… **å¥åº·æ•™è‚²**: é¢„é˜²ä¿å¥çŸ¥è¯†å……è¶³

### æ³¨æ„äº‹é¡¹
1. âš ï¸ **ä¸“ç§‘å±€é™**: æŸäº›ä¸“ç§‘é¢†åŸŸè¦†ç›–ä¸å‡
2. âš ï¸ **å„¿ç§‘å’¨è¯¢**: å„¿ç«¥ç–¾ç—…ä¿¡æ¯ç›¸å¯¹è¾ƒå°‘
3. âš ï¸ **æ€¥è¯Šæƒ…å†µ**: æ€¥æ•‘åŒ»å­¦å†…å®¹æœ‰é™
4. âš ï¸ **å¿ƒç†å¥åº·**: ç²¾ç¥å¿ƒç†é—®é¢˜è¦†ç›–ä¸è¶³

## ç»“è®º

åä½—æ•°æ®é›†æ˜¯ä¸€ä¸ª**é«˜è´¨é‡çš„ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®é›†**ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **è¦†ç›–é¢å¹¿**: æ¶µç›–ä¸»è¦åŒ»å­¦é¢†åŸŸ
2. **è´¨é‡è¾ƒé«˜**: ç­”æ¡ˆç»“æ„åŒ–ç¨‹åº¦å¥½ï¼Œä¿¡æ¯å‡†ç¡®
3. **å®ç”¨æ€§å¼º**: è´´è¿‘å®é™…åŒ»ç–—å’¨è¯¢éœ€æ±‚
4. **ä¸­æ–‡ä¼˜åŠ¿**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡åŒ»å­¦é—®ç­”ä¼˜åŒ–

**æ€»ä½“è¯„ä»·**: é€‚åˆæ„å»ºä¸­æ–‡åŒ»å­¦é—®ç­”ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯é¢å‘å¸¸è§ç–¾ç—…å’¨è¯¢çš„RAGåº”ç”¨ã€‚
"""

    # ä¿å­˜æŠ¥å‘Š
    try:
        with open("huatuo_dataset_analysis_report.md", "w", encoding="utf-8") as f:
            f.write(report_content)
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: huatuo_dataset_analysis_report.md")
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

def extract_cardiovascular_data():
    """æŠ½å–å¿ƒè¡€ç®¡ç›¸å…³æ•°æ®"""
    print("ğŸ’“ æŠ½å–å¿ƒè¡€ç®¡ç›¸å…³æ•°æ®")
    print("=" * 60)

    try:
        from data_processor import HuatuoDataProcessor

        # åŠ è½½æ•°æ®é›†
        processor = HuatuoDataProcessor()
        print("ğŸ“¥ æ­£åœ¨åŠ è½½åä½—æ•°æ®é›†...")

        # åŠ è½½å®Œæ•´æ•°æ®é›†è¿›è¡Œç­›é€‰
        processor.load_huatuo_dataset(split="train", sample_size=None)  # åŠ è½½å…¨éƒ¨æ•°æ®
        all_data = processor.process_qa_pairs()

        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_data)} æ¡æ•°æ®")

        # å®šä¹‰å¿ƒè¡€ç®¡ç›¸å…³å…³é”®è¯ï¼ˆæ›´å…¨é¢ï¼‰
        cardiovascular_keywords = [
            # åŸºç¡€å¿ƒè¡€ç®¡æœ¯è¯­
            "å¿ƒè„", "å¿ƒè¡€ç®¡", "å¿ƒè‚Œ", "å¿ƒæˆ¿", "å¿ƒå®¤", "å¿ƒè„ç—…",
            "å† å¿ƒç—…", "å† çŠ¶åŠ¨è„‰", "å¿ƒè‚Œæ¢—æ­»", "å¿ƒæ¢—", "å¿ƒç»ç—›",
            "å¿ƒå¾‹ä¸é½", "å¿ƒå¾‹å¤±å¸¸", "å¿ƒæ‚¸", "å¿ƒæ…Œ", "å¿ƒè·³",
            "å¿ƒåŠ¨è¿‡é€Ÿ", "å¿ƒåŠ¨è¿‡ç¼“", "æˆ¿é¢¤", "å®¤é¢¤",

            # è¡€å‹ç›¸å…³
            "é«˜è¡€å‹", "ä½è¡€å‹", "è¡€å‹", "æ”¶ç¼©å‹", "èˆ’å¼ å‹",
            "é«˜å‹", "ä½å‹", "è¡€å‹è®¡", "é™å‹",

            # è¡€ç®¡ç›¸å…³
            "åŠ¨è„‰", "é™è„‰", "è¡€ç®¡", "åŠ¨è„‰ç¡¬åŒ–", "åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–",
            "è¡€æ “", "æ “å¡", "è¡€ç®¡å µå¡", "è¡€ç®¡ç‹­çª„",
            "ä¸»åŠ¨è„‰", "è‚ºåŠ¨è„‰", "é¢ˆåŠ¨è„‰", "å† çŠ¶åŠ¨è„‰",

            # ç—‡çŠ¶ç›¸å…³
            "èƒ¸ç—›", "èƒ¸é—·", "æ°”çŸ­", "å‘¼å¸å›°éš¾", "å¿ƒå‰åŒºç–¼ç—›",
            "å·¦èƒ¸ç—›", "å¿ƒå£ç—›", "èƒ¸éƒ¨ä¸é€‚",

            # æ£€æŸ¥ç›¸å…³
            "å¿ƒç”µå›¾", "å¿ƒè„å½©è¶…", "å† è„‰é€ å½±", "å¿ƒè„CT",
            "å¿ƒè‚Œé…¶", "è‚Œé’™è›‹ç™½", "BNP", "NT-proBNP",

            # æ²»ç–—ç›¸å…³
            "å¿ƒè„æ”¯æ¶", "æ­æ¡¥æ‰‹æœ¯", "å¿ƒè„æ‰‹æœ¯", "èµ·æå™¨",
            "ç¡é…¸ç”˜æ²¹", "é˜¿å¸åŒ¹æ—", "ä»–æ±€", "ACEI", "ARB",
            "Î²å—ä½“é˜»æ»å‰‚", "é’™é€šé“é˜»æ»å‰‚",

            # ç–¾ç—…åç§°
            "å¿ƒåŠ›è¡°ç«­", "å¿ƒè¡°", "å¿ƒè‚Œç—…", "å¿ƒåŒ…ç‚", "å¿ƒå†…è†œç‚",
            "é£æ¹¿æ€§å¿ƒè„ç—…", "å…ˆå¤©æ€§å¿ƒè„ç—…", "ç“£è†œç—…",
            "å¿ƒè„ç“£è†œ", "äºŒå°–ç“£", "ä¸»åŠ¨è„‰ç“£", "ä¸‰å°–ç“£",

            # å…¶ä»–ç›¸å…³
            "å¿ƒè„åº·å¤", "å¿ƒè„ç§»æ¤", "å¿ƒè„çŒæ­»", "å¿ƒæºæ€§",
            "å¿ƒè¡€ç®¡å±é™©å› ç´ ", "å¿ƒè¡€ç®¡äº‹ä»¶"
        ]

        # ç­›é€‰å¿ƒè¡€ç®¡ç›¸å…³æ•°æ®
        cardiovascular_data = []

        for item in all_data:
            question = item['question'].lower()
            answer = item['answer'].lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿ƒè¡€ç®¡å…³é”®è¯
            is_cardiovascular = False
            matched_keywords = []

            for keyword in cardiovascular_keywords:
                if keyword in question or keyword in answer:
                    is_cardiovascular = True
                    matched_keywords.append(keyword)

            if is_cardiovascular:
                # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ä¿¡æ¯
                item['matched_keywords'] = list(set(matched_keywords))
                item['keyword_count'] = len(matched_keywords)
                cardiovascular_data.append(item)

        print(f"ğŸ’“ æˆåŠŸç­›é€‰å‡º {len(cardiovascular_data)} æ¡å¿ƒè¡€ç®¡ç›¸å…³æ•°æ®")
        print(f"ğŸ“Š å æ€»æ•°æ®çš„ {len(cardiovascular_data)/len(all_data)*100:.2f}%")

        # ä¿å­˜å¿ƒè¡€ç®¡æ•°æ®
        save_cardiovascular_data(cardiovascular_data)

        # åˆ†æå¿ƒè¡€ç®¡æ•°æ®ç‰¹å¾
        analyze_cardiovascular_data(cardiovascular_data)

        return cardiovascular_data

    except Exception as e:
        print(f"âŒ å¿ƒè¡€ç®¡æ•°æ®æŠ½å–å¤±è´¥: {e}")
        return None

def save_cardiovascular_data(data: List[Dict[str, Any]]):
    """ä¿å­˜å¿ƒè¡€ç®¡æ•°æ®åˆ°æ–‡ä»¶"""

    try:
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        output_file = "cardiovascular_qa_data.json"

        # å‡†å¤‡ä¿å­˜çš„æ•°æ®ï¼ˆç§»é™¤ä¸å¿…è¦çš„å­—æ®µï¼‰
        save_data = []
        for item in data:
            save_item = {
                "question": item['question'],
                "answer": item['answer'],
                "matched_keywords": item.get('matched_keywords', []),
                "keyword_count": item.get('keyword_count', 0)
            }
            save_data.append(save_item)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ å¿ƒè¡€ç®¡æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

        # åŒæ—¶ä¿å­˜ä¸ºç®€åŒ–çš„æ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹
        text_file = "cardiovascular_qa_data.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write("å¿ƒè¡€ç®¡ç›¸å…³é—®ç­”æ•°æ®\n")
            f.write("=" * 50 + "\n\n")

            for i, item in enumerate(data[:100], 1):  # åªä¿å­˜å‰100æ¡åˆ°æ–‡æœ¬æ–‡ä»¶
                f.write(f"ã€é—®ç­”å¯¹ {i}ã€‘\n")
                f.write(f"é—®é¢˜: {item['question']}\n")
                f.write(f"ç­”æ¡ˆ: {item['answer']}\n")
                f.write(f"åŒ¹é…å…³é”®è¯: {', '.join(item.get('matched_keywords', []))}\n")
                f.write("-" * 50 + "\n\n")

        print(f"ğŸ“„ å‰100æ¡æ•°æ®çš„æ–‡æœ¬ç‰ˆæœ¬å·²ä¿å­˜åˆ°: {text_file}")

    except Exception as e:
        print(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {e}")

def analyze_cardiovascular_data(data: List[Dict[str, Any]]):
    """åˆ†æå¿ƒè¡€ç®¡æ•°æ®ç‰¹å¾"""

    print(f"\nğŸ“Š å¿ƒè¡€ç®¡æ•°æ®åˆ†æ")
    print("=" * 40)

    # ç»Ÿè®¡å…³é”®è¯é¢‘æ¬¡
    keyword_counter = Counter()
    for item in data:
        for keyword in item.get('matched_keywords', []):
            keyword_counter[keyword] += 1

    print(f"\nğŸ”¤ é«˜é¢‘å¿ƒè¡€ç®¡å…³é”®è¯ (Top 20):")
    for keyword, count in keyword_counter.most_common(20):
        print(f"   {keyword}: {count:,} æ¬¡")

    # åˆ†æé—®é¢˜é•¿åº¦åˆ†å¸ƒ
    question_lengths = [len(item['question']) for item in data]
    answer_lengths = [len(item['answer']) for item in data]

    print(f"\nğŸ“ æ•°æ®é•¿åº¦ç»Ÿè®¡:")
    print(f"   é—®é¢˜å¹³å‡é•¿åº¦: {sum(question_lengths)/len(question_lengths):.1f} å­—ç¬¦")
    print(f"   ç­”æ¡ˆå¹³å‡é•¿åº¦: {sum(answer_lengths)/len(answer_lengths):.1f} å­—ç¬¦")
    print(f"   é—®é¢˜é•¿åº¦èŒƒå›´: {min(question_lengths)} - {max(question_lengths)} å­—ç¬¦")
    print(f"   ç­”æ¡ˆé•¿åº¦èŒƒå›´: {min(answer_lengths)} - {max(answer_lengths)} å­—ç¬¦")

    # åˆ†æå¿ƒè¡€ç®¡å­é¢†åŸŸåˆ†å¸ƒ
    cardiovascular_subdomains = {
        "å† å¿ƒç—…": ["å† å¿ƒç—…", "å† çŠ¶åŠ¨è„‰", "å¿ƒè‚Œæ¢—æ­»", "å¿ƒæ¢—", "å¿ƒç»ç—›"],
        "é«˜è¡€å‹": ["é«˜è¡€å‹", "è¡€å‹", "é™å‹", "æ”¶ç¼©å‹", "èˆ’å¼ å‹"],
        "å¿ƒå¾‹å¤±å¸¸": ["å¿ƒå¾‹ä¸é½", "å¿ƒå¾‹å¤±å¸¸", "å¿ƒæ‚¸", "å¿ƒæ…Œ", "æˆ¿é¢¤"],
        "å¿ƒåŠ›è¡°ç«­": ["å¿ƒåŠ›è¡°ç«­", "å¿ƒè¡°", "å¿ƒåŠŸèƒ½ä¸å…¨"],
        "å¿ƒè„æ£€æŸ¥": ["å¿ƒç”µå›¾", "å¿ƒè„å½©è¶…", "å¿ƒè‚Œé…¶", "è‚Œé’™è›‹ç™½"],
        "å¿ƒè„ç—‡çŠ¶": ["èƒ¸ç—›", "èƒ¸é—·", "æ°”çŸ­", "å¿ƒå‰åŒºç–¼ç—›"],
        "å¿ƒè„æ²»ç–—": ["å¿ƒè„æ”¯æ¶", "æ­æ¡¥æ‰‹æœ¯", "ç¡é…¸ç”˜æ²¹", "é˜¿å¸åŒ¹æ—"]
    }

    subdomain_counts = defaultdict(int)

    for item in data:
        question = item['question'].lower()
        answer = item['answer'].lower()

        for subdomain, keywords in cardiovascular_subdomains.items():
            for keyword in keywords:
                if keyword in question or keyword in answer:
                    subdomain_counts[subdomain] += 1
                    break

    print(f"\nğŸ¥ å¿ƒè¡€ç®¡å­é¢†åŸŸåˆ†å¸ƒ:")
    total_subdomain = sum(subdomain_counts.values())
    for subdomain, count in sorted(subdomain_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_subdomain) * 100 if total_subdomain > 0 else 0
        print(f"   {subdomain}: {count:,} æ¬¡ ({percentage:.1f}%)")

    # ç”Ÿæˆå¿ƒè¡€ç®¡æ•°æ®æŠ¥å‘Š
    generate_cardiovascular_report(data, keyword_counter, subdomain_counts)

def generate_cardiovascular_report(data: List[Dict[str, Any]],
                                 keyword_counter: Counter,
                                 subdomain_counts: Dict[str, int]):
    """ç”Ÿæˆå¿ƒè¡€ç®¡æ•°æ®ä¸“é¡¹æŠ¥å‘Š"""

    report_content = f"""# åä½—æ•°æ®é›† - å¿ƒè¡€ç®¡ä¸“é¡¹æ•°æ®åˆ†ææŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- **æŠ½å–æ—¶é—´**: {json.dumps({"timestamp": "2024"}, ensure_ascii=False)}
- **æ€»æ•°æ®é‡**: {len(data):,} æ¡å¿ƒè¡€ç®¡ç›¸å…³é—®ç­”å¯¹
- **æ•°æ®æ¥æº**: FreedomIntelligence/huatuo_knowledge_graph_qa
- **ç­›é€‰æ ‡å‡†**: åŒ…å«å¿ƒè¡€ç®¡ç›¸å…³å…³é”®è¯çš„é—®ç­”å¯¹

## æ•°æ®è´¨é‡åˆ†æ

### åŸºæœ¬ç»Ÿè®¡
- **é—®é¢˜å¹³å‡é•¿åº¦**: {sum(len(item['question']) for item in data)/len(data):.1f} å­—ç¬¦
- **ç­”æ¡ˆå¹³å‡é•¿åº¦**: {sum(len(item['answer']) for item in data)/len(data):.1f} å­—ç¬¦
- **å…³é”®è¯åŒ¹é…åº¦**: å¹³å‡æ¯æ¡æ•°æ®åŒ¹é… {sum(item.get('keyword_count', 0) for item in data)/len(data):.1f} ä¸ªå…³é”®è¯

### é«˜é¢‘å…³é”®è¯åˆ†æ
"""

    # æ·»åŠ å…³é”®è¯ç»Ÿè®¡
    for keyword, count in keyword_counter.most_common(15):
        report_content += f"- **{keyword}**: {count:,}æ¬¡\n"

    # æ·»åŠ å­é¢†åŸŸåˆ†æ
    report_content += f"\n## å¿ƒè¡€ç®¡å­é¢†åŸŸåˆ†å¸ƒ\n\n"
    total_subdomain = sum(subdomain_counts.values())

    for subdomain, count in sorted(subdomain_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_subdomain) * 100 if total_subdomain > 0 else 0
        report_content += f"- **{subdomain}**: {count:,}æ¬¡ ({percentage:.1f}%)\n"

    # æ·»åŠ æ•°æ®ç¤ºä¾‹
    report_content += f"\n## æ•°æ®ç¤ºä¾‹\n\n"
    for i, item in enumerate(data[:5], 1):
        report_content += f"### ç¤ºä¾‹ {i}\n"
        report_content += f"**é—®é¢˜**: {item['question']}\n\n"
        report_content += f"**ç­”æ¡ˆ**: {item['answer'][:200]}{'...' if len(item['answer']) > 200 else ''}\n\n"
        report_content += f"**åŒ¹é…å…³é”®è¯**: {', '.join(item.get('matched_keywords', []))}\n\n"
        report_content += "---\n\n"

    # æ·»åŠ åº”ç”¨å»ºè®®
    report_content += f"""## åº”ç”¨å»ºè®®

### é€‚ç”¨åœºæ™¯
1. âœ… **å¿ƒè¡€ç®¡ç–¾ç—…å’¨è¯¢ç³»ç»Ÿ**: è¦†ç›–å¸¸è§å¿ƒè¡€ç®¡é—®é¢˜
2. âœ… **åŒ»å­¦æ•™è‚²å¹³å°**: å¿ƒè¡€ç®¡çŸ¥è¯†é—®ç­”
3. âœ… **å¥åº·ç®¡ç†åº”ç”¨**: å¿ƒè¡€ç®¡å¥åº·æŒ‡å¯¼
4. âœ… **åŒ»ç–—RAGç³»ç»Ÿ**: å¿ƒè¡€ç®¡ä¸“ä¸šçŸ¥è¯†åº“

### æ•°æ®ç‰¹ç‚¹
1. **è¦†ç›–å…¨é¢**: æ¶µç›–å¿ƒè¡€ç®¡ç–¾ç—…çš„ä¸»è¦æ–¹é¢
2. **å®ç”¨æ€§å¼º**: è´´è¿‘å®é™…ä¸´åºŠå’¨è¯¢
3. **è´¨é‡è¾ƒé«˜**: ç­”æ¡ˆè¯¦ç»†ï¼Œä¿¡æ¯å‡†ç¡®
4. **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡åŒ»å­¦è¡¨è¾¾

### ä½¿ç”¨å»ºè®®
1. **é¢„å¤„ç†**: å»ºè®®è¿›è¡Œå»é‡å’Œè´¨é‡ç­›é€‰
2. **åˆ†ç±»æ ‡æ³¨**: å¯æŒ‰å­é¢†åŸŸè¿›ä¸€æ­¥åˆ†ç±»
3. **å¢å¼ºå¤„ç†**: å¯ç»“åˆå…¶ä»–åŒ»å­¦çŸ¥è¯†åº“å¢å¼º
4. **å®šæœŸæ›´æ–°**: å»ºè®®å®šæœŸæ›´æ–°åŒ»å­¦çŸ¥è¯†

## æ€»ç»“

æœ¬æ¬¡ä»åä½—æ•°æ®é›†ä¸­æˆåŠŸæŠ½å–äº† **{len(data):,} æ¡å¿ƒè¡€ç®¡ç›¸å…³é—®ç­”å¯¹**ï¼Œæ•°æ®è´¨é‡è‰¯å¥½ï¼Œè¦†ç›–é¢å¹¿æ³›ã€‚

**ä¸»è¦ä¼˜åŠ¿**:
- æ¶µç›–å¿ƒè¡€ç®¡ç–¾ç—…çš„ä¸»è¦é¢†åŸŸ
- é—®ç­”è´¨é‡è¾ƒé«˜ï¼Œä¿¡æ¯å‡†ç¡®
- é€‚åˆæ„å»ºå¿ƒè¡€ç®¡ä¸“ä¸šRAGç³»ç»Ÿ

**åº”ç”¨ä»·å€¼**:
- å¯ç›´æ¥ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…å’¨è¯¢ç³»ç»Ÿ
- é€‚åˆåŒ»å­¦æ•™è‚²å’Œå¥åº·ç®¡ç†åº”ç”¨
- ä¸ºå¿ƒè¡€ç®¡ä¸“ä¸šAIåŠ©æ‰‹æä¾›çŸ¥è¯†åŸºç¡€
"""

    # ä¿å­˜æŠ¥å‘Š
    try:
        with open("cardiovascular_data_report.md", "w", encoding="utf-8") as f:
            f.write(report_content)
        print(f"ğŸ“‹ å¿ƒè¡€ç®¡æ•°æ®åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: cardiovascular_data_report.md")
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” åä½—æ•°æ®é›†å†…å®¹åˆ†æå·¥å…·")
    print("=" * 60)

    import sys

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "cardiovascular":
        # æŠ½å–å¿ƒè¡€ç®¡æ•°æ®
        cardiovascular_data = extract_cardiovascular_data()

        if cardiovascular_data:
            print(f"\nğŸ’“ å¿ƒè¡€ç®¡æ•°æ®æŠ½å–å®Œæˆ!")
            print(f"âœ… æˆåŠŸæŠ½å– {len(cardiovascular_data):,} æ¡å¿ƒè¡€ç®¡ç›¸å…³é—®ç­”å¯¹")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: cardiovascular_qa_data.json")
            print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: cardiovascular_data_report.md")
        else:
            print("âŒ å¿ƒè¡€ç®¡æ•°æ®æŠ½å–å¤±è´¥")
    else:
        # æ‰§è¡Œå®Œæ•´åˆ†æ
        analysis_result = analyze_dataset_content()

        if analysis_result:
            # ç”ŸæˆæŠ¥å‘Š
            generate_analysis_report(analysis_result)

            print(f"\nğŸ¯ æ€»ç»“:")
            print(f"åä½—æ•°æ®é›†ä¸»è¦åå‘äº:")
            print(f"1. ğŸ“‹ å¸¸è§ç–¾ç—…å’¨è¯¢ (å¿ƒè¡€ç®¡ã€æ¶ˆåŒ–ã€å‘¼å¸ç³»ç»Ÿ)")
            print(f"2. ğŸ’Š ç—‡çŠ¶åˆ†æå’Œæ²»ç–—æŒ‡å¯¼")
            print(f"3. ğŸ¥ æˆäººåŒ»å­¦ (ç›¸å¯¹äºå„¿ç§‘)")
            print(f"4. ğŸ” å®ç”¨æ€§åŒ»å­¦é—®ç­” (è´´è¿‘æ—¥å¸¸å’¨è¯¢)")

            print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ 'python analyze_huatuo_dataset.py cardiovascular' å¯æŠ½å–å¿ƒè¡€ç®¡ä¸“é¡¹æ•°æ®")

        else:
            print("âŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†é…ç½®")

if __name__ == "__main__":
    main()
