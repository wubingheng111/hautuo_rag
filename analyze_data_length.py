#!/usr/bin/env python3
"""
åˆ†æå¿ƒè¡€ç®¡æ•°æ®çš„é•¿åº¦åˆ†å¸ƒ
"""
import json

def analyze_data_length():
    """åˆ†ææ•°æ®é•¿åº¦"""
    print("ğŸ“Š åˆ†æå¿ƒè¡€ç®¡æ•°æ®é•¿åº¦åˆ†å¸ƒ")
    print("=" * 50)
    
    try:
        with open('cardiovascular_qa_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"æ€»æ•°æ®é‡: {len(data):,} æ¡")
        
        # åˆ†æé•¿åº¦
        question_lengths = []
        answer_lengths = []
        combined_lengths = []
        
        for item in data[:5000]:  # åˆ†æå‰5000æ¡
            question = item['question']
            answer = item['answer']
            combined = f"é—®é¢˜: {question}\nç­”æ¡ˆ: {answer}"
            
            question_lengths.append(len(question))
            answer_lengths.append(len(answer))
            combined_lengths.append(len(combined))
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ é•¿åº¦ç»Ÿè®¡ (åŸºäºå‰5000æ¡æ•°æ®):")
        print(f"é—®é¢˜å¹³å‡é•¿åº¦: {sum(question_lengths)/len(question_lengths):.1f} å­—ç¬¦")
        print(f"ç­”æ¡ˆå¹³å‡é•¿åº¦: {sum(answer_lengths)/len(answer_lengths):.1f} å­—ç¬¦")
        print(f"ç»„åˆå¹³å‡é•¿åº¦: {sum(combined_lengths)/len(combined_lengths):.1f} å­—ç¬¦")
        
        print(f"\nğŸ“Š é•¿åº¦èŒƒå›´:")
        print(f"é—®é¢˜: {min(question_lengths)} - {max(question_lengths)} å­—ç¬¦")
        print(f"ç­”æ¡ˆ: {min(answer_lengths)} - {max(answer_lengths)} å­—ç¬¦")
        print(f"ç»„åˆ: {min(combined_lengths)} - {max(combined_lengths)} å­—ç¬¦")
        
        # é•¿åº¦åˆ†å¸ƒ
        print(f"\nğŸ“ˆ ç»„åˆæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ:")
        very_short = sum(1 for l in combined_lengths if l < 50)
        short = sum(1 for l in combined_lengths if 50 <= l < 150)
        medium = sum(1 for l in combined_lengths if 150 <= l < 300)
        long = sum(1 for l in combined_lengths if 300 <= l < 500)
        very_long = sum(1 for l in combined_lengths if l >= 500)
        
        total = len(combined_lengths)
        print(f"æçŸ­æ–‡æœ¬(<50å­—ç¬¦): {very_short:,} æ¡ ({very_short/total*100:.1f}%)")
        print(f"çŸ­æ–‡æœ¬(50-150å­—ç¬¦): {short:,} æ¡ ({short/total*100:.1f}%)")
        print(f"ä¸­ç­‰æ–‡æœ¬(150-300å­—ç¬¦): {medium:,} æ¡ ({medium/total*100:.1f}%)")
        print(f"é•¿æ–‡æœ¬(300-500å­—ç¬¦): {long:,} æ¡ ({long/total*100:.1f}%)")
        print(f"è¶…é•¿æ–‡æœ¬(>=500å­—ç¬¦): {very_long:,} æ¡ ({very_long/total*100:.1f}%)")
        
        # ä¸­ä½æ•°
        sorted_lengths = sorted(combined_lengths)
        median = sorted_lengths[len(sorted_lengths)//2]
        p75 = sorted_lengths[int(len(sorted_lengths)*0.75)]
        p90 = sorted_lengths[int(len(sorted_lengths)*0.90)]
        
        print(f"\nğŸ“Š é•¿åº¦åˆ†ä½æ•°:")
        print(f"ä¸­ä½æ•°(50%): {median} å­—ç¬¦")
        print(f"75åˆ†ä½æ•°: {p75} å­—ç¬¦")
        print(f"90åˆ†ä½æ•°: {p90} å­—ç¬¦")
        
        # å»ºè®®çš„chunkè®¾ç½®
        print(f"\nğŸ’¡ å»ºè®®çš„é…ç½®:")
        if median < 100:
            print(f"å»ºè®®CHUNK_SIZE: 200-300 (å½“å‰ä¸­ä½æ•°: {median})")
        elif median < 200:
            print(f"å»ºè®®CHUNK_SIZE: 300-400 (å½“å‰ä¸­ä½æ•°: {median})")
        else:
            print(f"å»ºè®®CHUNK_SIZE: 400-500 (å½“å‰ä¸­ä½æ•°: {median})")
        
        print(f"å»ºè®®CHUNK_OVERLAP: 20-50")
        print(f"å»ºè®®SIMILARITY_THRESHOLD: 0.3-0.5 (å½“å‰å¯èƒ½è¿‡é«˜)")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        print(f"\nğŸ“ æ•°æ®ç¤ºä¾‹:")
        for i, item in enumerate(data[:3], 1):
            combined = f"é—®é¢˜: {item['question']}\nç­”æ¡ˆ: {item['answer']}"
            print(f"\nç¤ºä¾‹ {i} (é•¿åº¦: {len(combined)} å­—ç¬¦):")
            print(f"é—®é¢˜: {item['question']}")
            print(f"ç­”æ¡ˆ: {item['answer'][:100]}{'...' if len(item['answer']) > 100 else ''}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    analyze_data_length()
